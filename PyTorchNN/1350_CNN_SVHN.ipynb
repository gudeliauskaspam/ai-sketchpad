{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks presents **ConvNet** in PyTorch used to solve **Street View House Numbers** task.\n",
    "\n",
    "This is replication of _Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks_\n",
    "\n",
    "**Contents**\n",
    "* [Imports](#Imports)\n",
    "* [Dataset](#Dataset)\n",
    "* [Model](#Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and extract [SVHN](http://ufldl.stanford.edu/housenumbers/) dataset in **Format 1** (train.tar.gz, test.tar.gz, extra.tar.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = '/home/marcin/Datasets/SVHN'  # .../train/1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import h5py  # required to open .mat files in SVHN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = '/home/marcin/Datasets/SVHN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = pathlib.Path(dataset_location)\n",
    "assert os.path.isfile(dataset_path / 'extra/1.png')\n",
    "assert os.path.isfile(dataset_path / 'train/1.png')\n",
    "assert os.path.isfile(dataset_path / 'test/1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to read `.mat` files with labels and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_name(f, index):\n",
    "    \"\"\"Decode string from HDF5 file.\"\"\"\n",
    "    assert isinstance(f, h5py.File)\n",
    "    assert index == int(index)\n",
    "    ref = f['/digitStruct/name'][index][0]\n",
    "    return ''.join(chr(v[0]) for v in f[ref])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_digits_raw(f, index):\n",
    "    \"\"\"Decode digits and bounding boxes from HDF5 file.\"\"\"\n",
    "    assert isinstance(f, h5py.File)\n",
    "    assert index == int(index)\n",
    "    \n",
    "    ref = f['/digitStruct/bbox'][index].item()\n",
    "    ddd = {}\n",
    "    for key in ['label', 'left', 'top', 'width', 'height']:\n",
    "        dset = f[ref][key]\n",
    "        if len(dset) == 1:\n",
    "            ddd[key] = [ int(dset[0][0]) ]\n",
    "        else:\n",
    "            ddd[key] = []\n",
    "            for i in range(len(dset)):\n",
    "                ref2 = dset[i][0]\n",
    "                ddd[key].append( int(f[ref2][0][0]) )\n",
    "    return ddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(ddict):\n",
    "    \"\"\"Convert raw digit info into len-5 label and single bounding box\"\"\"\n",
    "    assert isinstance(ddict, dict)\n",
    "    \n",
    "    # construct proper label for NN training\n",
    "    # image '210' -> [3, 2, 1, 10, 0, 0]\n",
    "    #                 ^  ^  ^  ^   ^--^-- \"0, 0\" pad with '0' (no digit)\n",
    "    #                 |  ---------------- \"210\" house number, 0 encoded as 10\n",
    "    #                 ------------------- \"3\" is number of digits\n",
    "    label = ddict['label'].copy()\n",
    "    label = [len(label)] + label + [0]*(5-len(label))\n",
    "    \n",
    "    left = min(ddict['left'])\n",
    "    top = min(ddict['top'])\n",
    "    right = max(l+w for l, w in zip(ddict['left'], ddict['width']))\n",
    "    bottom = max(t+h for t, h in zip(ddict['top'], ddict['height']))\n",
    "    return tuple(label), (left, top, right, bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mat_file(filepath):\n",
    "    \"\"\"Open .mat file and read all the metadata.\"\"\"\n",
    "    assert isinstance(filepath, (str, pathlib.PosixPath))\n",
    "    \n",
    "    print(filepath)\n",
    "    \n",
    "    meta = {'names':[], 'labels':[], 'bboxes':[]}\n",
    "    with h5py.File(filepath) as f:\n",
    "        length = len(f['/digitStruct/name'])\n",
    "        for i in range(10): # length):\n",
    "            name = read_name(f, i)\n",
    "            ddict = read_digits_raw(f, i)\n",
    "            label, bbox = get_label(ddict)\n",
    "            meta['names'].append(name)\n",
    "            meta['labels'].append(label)\n",
    "            meta['bboxes'].append(bbox)\n",
    "            if i % 1000 == 0 or i == length-1:\n",
    "                print(f'{i:6d} / {length}')\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_or_generate(name):\n",
    "    \"\"\"Either load .pkl, or if doesn't exit generate it and open.\"\"\"\n",
    "    assert name in ('extra', 'test', 'train')\n",
    "    \n",
    "    fname = name+'.pkl'\n",
    "    if os.path.exists(dataset_path / fname):\n",
    "        with open(dataset_path / fname, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "            print(f'Loaded:{fname}')\n",
    "    else:\n",
    "        print(f'Generating {fname}:')\n",
    "        meta = read_mat_file(dataset_path / name / 'digitStruct.mat')\n",
    "        with open(dataset_path / fname, 'wb') as f:\n",
    "            pickle.dump(meta, f)\n",
    "    \n",
    "    return meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** this may take <u>around one hour</u> to complete, but only on the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:extra.pkl\n",
      "Loaded:test.pkl\n",
      "Loaded:train.pkl\n"
     ]
    }
   ],
   "source": [
    "# Convert label/bbox data to friendly format\n",
    "extra_meta = open_or_generate('extra')\n",
    "test_meta = open_or_generate('test')\n",
    "train_meta = open_or_generate('train')\n",
    "\n",
    "# Add folder information\n",
    "extra_meta['folders'] = ['extra'] * len(extra_meta['names'])\n",
    "test_meta['folders'] = ['test'] * len(test_meta['names'])\n",
    "train_meta['folders'] = ['train'] * len(train_meta['names'])\n",
    "\n",
    "# Add 'extra' to 'train' data\n",
    "train_meta['names'].extend(extra_meta['names'])\n",
    "train_meta['labels'].extend(extra_meta['labels'])\n",
    "train_meta['bboxes'].extend(extra_meta['bboxes'])\n",
    "train_meta['folders'].extend(extra_meta['folders'])\n",
    "del extra_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVHNDataset: #(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, metadata, transforms=None):\n",
    "        assert isinstance(dataset_path, (str, pathlib.PosixPath))\n",
    "        assert isinstance(metadata, dict)\n",
    "        assert set(metadata.keys()) == {'bboxes','folders','labels','names'}\n",
    "        assert len(metadata['names']) == len(metadata['labels'])\n",
    "        assert len(metadata['names']) == len(metadata['bboxes'])\n",
    "        assert len(metadata['names']) == len(metadata['folders'])\n",
    "        assert transforms is None or \\\n",
    "               isinstance(transforms, torchvision.transforms.Compose)\n",
    "        \n",
    "        self.dataset_path = pathlib.PosixPath(dataset_path)\n",
    "        self.metadata = metadata\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata['names'])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.metadata['names'][index]      # e.g. '1.png'\n",
    "        image_folder = self.metadata['folders'][index]  # e.g. 'test'\n",
    "        label = self.metadata['labels'][index]  # [1, 2, 10, 0, 0]\n",
    "        bbox = self.metadata['bboxes'][index]   # [left, top, right, bottom]\n",
    "        \n",
    "        # Figure out crop box\n",
    "        left, top, right, bottom = bbox\n",
    "        width, height = right - left, bottom - top\n",
    "        crop_left =   int(round(left   - .15*width))\n",
    "        crop_top =    int(round(top    - .15*height))\n",
    "        crop_right =  int(round(right  + .15*width))\n",
    "        crop_bottom = int(round(bottom + .15*height))\n",
    "        \n",
    "        img = PIL.Image.open(self.dataset_path / image_folder / image_name)\n",
    "        img2 = img.crop(box=(crop_left, crop_top, crop_right, crop_bottom))\n",
    "        res = img2.resize((64, 64))\n",
    "        if self.transforms is not None:\n",
    "            res = self.transforms(res)\n",
    "        \n",
    "        return res, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create temporary dataset to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SVHNDataset(dataset_path, train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: (3, 3, 3, 2, 0, 0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F1FCB31B978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = dataset[200000]\n",
    "print(f'Label: {label}')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create actual datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop([54, 54]),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "transforms_valid = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.CenterCrop([54, 54]),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset_train = SVHNDataset(dataset_path, train_meta, transforms_train)\n",
    "dataset_valid = SVHNDataset(dataset_path, test_meta, transforms_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And dataloaders as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=0, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset_valid, batch_size=128, shuffle=False,\n",
    "    num_workers=0, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVHNModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        def _block(in_channels, out_channels, stride):\n",
    "            \"\"\"Helper to build CNN blocks.\"\"\"\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                          kernel_size=5, padding=2),\n",
    "                nn.BatchNorm2d(num_features=out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=stride, padding=1),\n",
    "                nn.Dropout(0.2)\n",
    "            )\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.block1 = _block(in_channels=  3, out_channels= 48, stride=2)\n",
    "        self.block2 = _block(in_channels= 48, out_channels= 64, stride=1)\n",
    "        self.block3 = _block(in_channels= 64, out_channels=128, stride=2)\n",
    "        self.block4 = _block(in_channels=128, out_channels=160, stride=1)\n",
    "        self.block5 = _block(in_channels=160, out_channels=192, stride=2)\n",
    "        self.block6 = _block(in_channels=192, out_channels=192, stride=1)\n",
    "        self.block7 = _block(in_channels=192, out_channels=192, stride=2)\n",
    "        self.block8 = _block(in_channels=192, out_channels=192, stride=1)\n",
    "        self.fc1 = nn.Sequential(nn.Linear(192 * 7 * 7, 3072), nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(3072, 3072), nn.ReLU())\n",
    "        \n",
    "        self.length = nn.Sequential(nn.Linear(3072, 7))\n",
    "        self.digit1 = nn.Sequential(nn.Linear(3072, 11))\n",
    "        self.digit2 = nn.Sequential(nn.Linear(3072, 11))\n",
    "        self.digit3 = nn.Sequential(nn.Linear(3072, 11))\n",
    "        self.digit4 = nn.Sequential(nn.Linear(3072, 11))\n",
    "        self.digit5 = nn.Sequential(nn.Linear(3072, 11))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = x.view(x.size(0), 192*7*7)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        length = self.length(x)  #  logits!\n",
    "        digit1 = self.digit1(x)\n",
    "        digit2 = self.digit2(x)\n",
    "        digit3 = self.digit3(x)\n",
    "        digit4 = self.digit4(x)\n",
    "        digit5 = self.digit5(x)\n",
    "        \n",
    "        return length, digit1, digit2, digit3, digit4, digit5    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logits, targets):\n",
    "    \"\"\"Custom loss function.\n",
    "    \n",
    "    Params:\n",
    "        logits (list): with following members:\n",
    "            logits[0] (torch.Tensor): length, shape [n_batch, 7], LOGITS!\n",
    "            logits[1] (torch.Tensor): digit1, shape [n_batch, 11], LOGITS!\n",
    "            ...\n",
    "        targets (list): with members:\n",
    "            targets[0] (torch.Tensor): length target, shape [n_batch]\n",
    "            targets[1] (torch.Tensor): digit1 target, shape [n_batch]\n",
    "            ...\n",
    "    \"\"\"\n",
    "    length_ce = F.cross_entropy(logits[0], targets[0])\n",
    "    digit1_ce = F.cross_entropy(logits[1], targets[1])\n",
    "    digit2_ce = F.cross_entropy(logits[2], targets[2])\n",
    "    digit3_ce = F.cross_entropy(logits[3], targets[3])\n",
    "    digit4_ce = F.cross_entropy(logits[4], targets[4])\n",
    "    digit5_ce = F.cross_entropy(logits[5], targets[5])\n",
    "    loss = length_ce + digit1_ce + digit2_ce + \\\n",
    "           digit3_ce + digit4_ce + digit5_ce\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVHNModel()\n",
    "model.cuda()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=10000, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-30 09:39:39.310617: step=100 loss=8.016229629516602 lr=0.01 img/sec=566.4265987832299\n",
      "2019-08-30 09:39:44.529247: step=200 loss=7.1629838943481445 lr=0.01 img/sec=614.2883171798828\n",
      "2019-08-30 09:39:49.928340: step=300 loss=6.387057781219482 lr=0.01 img/sec=593.316767158393\n",
      "2019-08-30 09:39:55.260262: step=400 loss=6.254304885864258 lr=0.01 img/sec=601.2512616993328\n",
      "2019-08-30 09:40:00.688071: step=500 loss=6.888719081878662 lr=0.01 img/sec=590.7177923628342\n",
      "2019-08-30 09:40:06.146924: step=600 loss=6.200620651245117 lr=0.01 img/sec=587.6774621571692\n",
      "2019-08-30 09:40:11.635999: step=700 loss=5.269405841827393 lr=0.01 img/sec=584.3131170284663\n",
      "2019-08-30 09:40:16.907616: step=800 loss=5.345304012298584 lr=0.01 img/sec=608.0247120792075\n",
      "2019-08-30 09:40:22.141942: step=900 loss=5.738339900970459 lr=0.01 img/sec=612.718949615225\n",
      "2019-08-30 09:40:27.414312: step=1000 loss=5.879678249359131 lr=0.01 img/sec=608.0938838184846\n",
      "2019-08-30 09:40:32.821004: step=1100 loss=5.156015396118164 lr=0.01 img/sec=593.3868825944152\n",
      "2019-08-30 09:40:38.347412: step=1200 loss=5.8263163566589355 lr=0.01 img/sec=580.4055789933386\n",
      "2019-08-30 09:40:43.806032: step=1300 loss=5.3964009284973145 lr=0.01 img/sec=587.1632530899759\n",
      "2019-08-30 09:40:49.210549: step=1400 loss=4.464658737182617 lr=0.01 img/sec=592.7395288749457\n",
      "2019-08-30 09:40:54.565304: step=1500 loss=5.047371864318848 lr=0.01 img/sec=598.760096831077\n",
      "2019-08-30 09:41:00.019785: step=1600 loss=3.957214832305908 lr=0.01 img/sec=587.6210379948395\n",
      "2019-08-30 09:41:05.416311: step=1700 loss=4.195733547210693 lr=0.01 img/sec=594.2463476608071\n",
      "2019-08-30 09:41:10.856558: step=1800 loss=4.385459899902344 lr=0.01 img/sec=589.0344350040714\n",
      "2019-08-30 09:41:16.225751: step=1900 loss=2.82621693611145 lr=0.01 img/sec=596.5513662666493\n",
      "2019-08-30 09:41:21.615909: step=2000 loss=3.129693031311035 lr=0.01 img/sec=594.4500842816225\n",
      "2019-08-30 09:41:27.032650: step=2100 loss=2.8357479572296143 lr=0.01 img/sec=591.5222957482613\n",
      "2019-08-30 09:41:32.467386: step=2200 loss=3.287836790084839 lr=0.01 img/sec=589.6257153209688\n",
      "2019-08-30 09:41:37.916708: step=2300 loss=3.612572431564331 lr=0.01 img/sec=588.4352325136706\n",
      "2019-08-30 09:41:43.184451: step=2400 loss=2.994070291519165 lr=0.01 img/sec=608.9882506654768\n",
      "2019-08-30 09:41:48.528800: step=2500 loss=2.713578224182129 lr=0.01 img/sec=600.108121444231\n",
      "2019-08-30 09:41:53.871296: step=2600 loss=2.799665927886963 lr=0.01 img/sec=600.384884448556\n",
      "2019-08-30 09:41:59.127016: step=2700 loss=2.1876916885375977 lr=0.01 img/sec=609.8302742101764\n",
      "2019-08-30 09:42:04.383345: step=2800 loss=2.4312632083892822 lr=0.01 img/sec=609.8724766962473\n",
      "2019-08-30 09:42:09.609893: step=2900 loss=1.2779287099838257 lr=0.01 img/sec=613.5424392550933\n",
      "2019-08-30 09:42:14.917016: step=3000 loss=1.890001654624939 lr=0.01 img/sec=604.2264786501005\n",
      "2019-08-30 09:42:20.246332: step=3100 loss=1.1227147579193115 lr=0.01 img/sec=601.4960574273091\n",
      "2019-08-30 09:42:25.556329: step=3200 loss=1.998724102973938 lr=0.01 img/sec=603.8642114865268\n",
      "2019-08-30 09:42:30.812399: step=3300 loss=2.161855697631836 lr=0.01 img/sec=610.0975258903782\n",
      "2019-08-30 09:42:36.104167: step=3400 loss=1.4289816617965698 lr=0.01 img/sec=605.7051604633638\n",
      "2019-08-30 09:42:41.442018: step=3500 loss=1.305215835571289 lr=0.01 img/sec=601.4173561627256\n",
      "2019-08-30 09:42:46.709544: step=3600 loss=2.167470932006836 lr=0.01 img/sec=608.7356633793538\n",
      "2019-08-30 09:42:51.983757: step=3700 loss=1.5482114553451538 lr=0.01 img/sec=608.4048964594923\n",
      "2019-08-30 09:42:57.286534: step=3800 loss=1.4916577339172363 lr=0.01 img/sec=605.1023224970455\n",
      "2019-08-30 09:43:02.576866: step=3900 loss=2.288196325302124 lr=0.01 img/sec=606.4374912853069\n",
      "2019-08-30 09:43:07.883342: step=4000 loss=0.7449666261672974 lr=0.01 img/sec=604.0953968384779\n",
      "2019-08-30 09:43:13.164661: step=4100 loss=1.375598669052124 lr=0.01 img/sec=607.4132920045146\n",
      "2019-08-30 09:43:18.437357: step=4200 loss=0.560960054397583 lr=0.01 img/sec=608.4430955091407\n",
      "2019-08-30 09:43:23.794334: step=4300 loss=0.6455767154693604 lr=0.01 img/sec=598.5282253065692\n",
      "2019-08-30 09:43:29.142459: step=4400 loss=1.5708030462265015 lr=0.01 img/sec=599.4230578221254\n",
      "2019-08-30 09:43:34.587103: step=4500 loss=1.504927158355713 lr=0.01 img/sec=588.9919395997016\n",
      "2019-08-30 09:43:39.930470: step=4600 loss=0.6214961409568787 lr=0.01 img/sec=600.2496386679295\n",
      "2019-08-30 09:43:45.149920: step=4700 loss=0.364126980304718 lr=0.01 img/sec=614.2667820164063\n",
      "2019-08-30 09:43:50.266692: step=4800 loss=1.180595874786377 lr=0.01 img/sec=626.8970378683608\n",
      "2019-08-30 09:43:54.892270: step=4900 loss=0.6683881878852844 lr=0.01 img/sec=694.2261564554246\n",
      "2019-08-30 09:44:00.231630: step=5000 loss=1.5478239059448242 lr=0.01 img/sec=600.8612131701589\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-19f1511f2471>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/svhn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/svhn/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ea4faa371143>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mcrop_bottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.15\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mimage_folder\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mimg2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_top\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_bottom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/svhn/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2777\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2779\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2781\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_every = 100\n",
    "\n",
    "step = 0\n",
    "time_start = time.time()\n",
    "\n",
    "for epoch in range(1):\n",
    "    \n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images = images.cuda()\n",
    "        targets = [t.cuda() for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = loss_function(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "                \n",
    "        step += 1\n",
    "        \n",
    "        if step % print_every == 0:\n",
    "            dtnow = datetime.datetime.now()\n",
    "            lr = scheduler.get_lr()[0]\n",
    "            dtime = time.time() - time_start\n",
    "            img_per_sec = batch_size * print_every / dtime\n",
    "            print(f'{dtnow}: step={step} loss={loss.item()} lr={lr} img/sec={img_per_sec}')\n",
    "            time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:svhn]",
   "language": "python",
   "name": "conda-env-svhn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
