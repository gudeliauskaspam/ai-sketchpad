{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents **LSTM** network with character-wise input trained on Shakespeare plays.\n",
    "\n",
    "Dataset file is included in this repo and consists of all works of Shakespeare concatenated together (4.6MB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = '../Datasets/shakespeare/shakespeare_input.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n"
     ]
    }
   ],
   "source": [
    "with open(dataset_location, 'r') as f:\n",
    "    text = f.read()\n",
    "print(text[:173])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discard bit at the end such that text is divisible by 1024. This allow for batch sizes [1, 2, 4, 8, 16, 32, ..., 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1024 = len(text) % 1024\n",
    "text = text[:-mod1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 696165), ('e', 386342), ('t', 272672), ('o', 268956), ('a', 230593)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = collections.Counter(text).most_common()\n",
    "tokens[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i2c: {0: ' ', 1: 'e', 2: 't', 3: 'o', 4: 'a', 5: 'h', 6: 's', 7: 'n', 8: 'r', 9: 'i', 10: '\\n', 11: 'l', 12: 'd', 13: 'u', 14: 'm', 15: 'y', 16: ',', 17: 'w', 18: 'f', 19: 'c', 20: 'g', 21: 'I', 22: ':', 23: 'b', 24: 'p', 25: 'A', 26: '.', 27: 'v', 28: 'T', 29: 'k', 30: \"'\", 31: 'S', 32: 'E', 33: 'O', 34: 'N', 35: 'R', 36: 'L', 37: ';', 38: 'C', 39: 'H', 40: 'W', 41: 'M', 42: 'U', 43: 'B', 44: 'D', 45: '?', 46: 'F', 47: '!', 48: '-', 49: 'G', 50: 'P', 51: 'Y', 52: 'K', 53: 'V', 54: 'j', 55: 'q', 56: 'x', 57: 'J', 58: 'z', 59: 'Q', 60: 'Z', 61: 'X', 62: '3', 63: '&', 64: '[', 65: ']', 66: '$'}\n",
      "c2i: {' ': 0, 'e': 1, 't': 2, 'o': 3, 'a': 4, 'h': 5, 's': 6, 'n': 7, 'r': 8, 'i': 9, '\\n': 10, 'l': 11, 'd': 12, 'u': 13, 'm': 14, 'y': 15, ',': 16, 'w': 17, 'f': 18, 'c': 19, 'g': 20, 'I': 21, ':': 22, 'b': 23, 'p': 24, 'A': 25, '.': 26, 'v': 27, 'T': 28, 'k': 29, \"'\": 30, 'S': 31, 'E': 32, 'O': 33, 'N': 34, 'R': 35, 'L': 36, ';': 37, 'C': 38, 'H': 39, 'W': 40, 'M': 41, 'U': 42, 'B': 43, 'D': 44, '?': 45, 'F': 46, '!': 47, '-': 48, 'G': 49, 'P': 50, 'Y': 51, 'K': 52, 'V': 53, 'j': 54, 'q': 55, 'x': 56, 'J': 57, 'z': 58, 'Q': 59, 'Z': 60, 'X': 61, '3': 62, '&': 63, '[': 64, ']': 65, '$': 66}\n"
     ]
    }
   ],
   "source": [
    "i2c = {i : c for i, (c, n) in enumerate(tokens)}\n",
    "c2i = {c : i for i, c in i2c.items()}\n",
    "print('i2c:', i2c)\n",
    "print('c2i:', c2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode text as tokens, reshape to batches, convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [[46  9  8 ...  3  2  0]\n",
      " [ 9  7  0 ... 30  6 16]\n",
      " [ 0 17  4 ...  1 19  3]\n",
      " ...\n",
      " [43  8  3 ...  0  9  2]\n",
      " [ 0  9  7 ...  5  1  8]\n",
      " [ 0  2  5 ...  0  4  8]]\n",
      "shape: (128, 35728)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "data = np.array([c2i[c] for c in text])\n",
    "data = data.reshape((batch_size, -1))\n",
    "print('data: ', data)\n",
    "print('shape:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: (128, 32155)\n",
      "valid_data: (128, 3573)\n"
     ]
    }
   ],
   "source": [
    "split_index = int(data.shape[1]*.9)  # 90% train, 10% valid\n",
    "train_data, valid_data = np.split(data, [split_index], axis=1)\n",
    "print('train_data:', train_data.shape)\n",
    "print('valid_data:', valid_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: torch.Size([128, 32155])\n",
      "valid_x: torch.Size([128, 3573])\n"
     ]
    }
   ],
   "source": [
    "train_x = torch.tensor(train_data).to(device)\n",
    "valid_x = torch.tensor(valid_data).to(device)\n",
    "print('train_x:', train_x.shape)\n",
    "print('valid_x:', valid_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, nb_layers, n_in, n_embed, n_hid, n_out, dropout):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=n_in, embedding_dim=n_embed)\n",
    "        self.lstm = nn.LSTM(input_size=n_embed, hidden_size=n_hid, num_layers=nb_layers,\n",
    "                           batch_first=True, dropout=dropout)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "        self.fc = nn.Linear(in_features=n_hid, out_features=n_out)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)                   # shape [n_batch, n_seq, n_embed]\n",
    "        x, hidden = self.lstm(x, hidden)    # shape [n_batch, n_seq, n_hid]\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)                      # shape [n_batch, n_seq, n_out]\n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_layers = 2\n",
    "n_in = len(i2c)\n",
    "n_seq = 256\n",
    "n_embed = 50\n",
    "n_hid = 64\n",
    "n_out = len(i2c)\n",
    "dropout = .5\n",
    "\n",
    "model = CharRNN(nb_layers, n_in, n_embed, n_hid, n_out, dropout)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0  0.0%     loss: 4.2041\n",
      "Epoch:   0  0.8%     loss: 4.1938\n",
      "Epoch:   0  1.6%     loss: 4.1847\n",
      "Epoch:   0  2.4%     loss: 4.1740\n",
      "Epoch:   0  3.2%     loss: 4.1627\n",
      "Epoch:   0  4.0%     loss: 4.1503\n",
      "Epoch:   0  4.8%     loss: 4.1368\n",
      "Epoch:   0  5.6%     loss: 4.1208\n",
      "Epoch:   0  6.4%     loss: 4.1018\n",
      "Epoch:   0  7.2%     loss: 4.0794\n",
      "Epoch:   0  8.0%     loss: 4.0512\n",
      "Epoch:   0  8.8%     loss: 4.0220\n",
      "Epoch:   0  9.6%     loss: 3.9798\n",
      "Epoch:   0 10.4%     loss: 3.9334\n",
      "Epoch:   0 11.1%     loss: 3.8809\n",
      "Epoch:   0 11.9%     loss: 3.8197\n",
      "Epoch:   0 12.7%     loss: 3.7596\n",
      "Epoch:   0 13.5%     loss: 3.7039\n",
      "Epoch:   0 14.3%     loss: 3.6642\n",
      "Epoch:   0 15.1%     loss: 3.6185\n",
      "Epoch:   0 15.9%     loss: 3.5918\n",
      "Epoch:   0 16.7%     loss: 3.5640\n",
      "Epoch:   0 17.5%     loss: 3.5238\n",
      "Epoch:   0 18.3%     loss: 3.5069\n",
      "Epoch:   0 19.1%     loss: 3.4970\n",
      "Epoch:   0 19.9%     loss: 3.4516\n",
      "Epoch:   0 20.7%     loss: 3.4465\n",
      "Epoch:   0 21.5%     loss: 3.4399\n",
      "Epoch:   0 22.3%     loss: 3.4348\n",
      "Epoch:   0 23.1%     loss: 3.4461\n",
      "Epoch:   0 23.9%     loss: 3.4273\n",
      "Epoch:   0 24.7%     loss: 3.4605\n",
      "Epoch:   0 25.5%     loss: 3.4435\n",
      "Epoch:   0 26.3%     loss: 3.4314\n",
      "Epoch:   0 27.1%     loss: 3.4341\n",
      "Epoch:   0 27.9%     loss: 3.4417\n",
      "Epoch:   0 28.7%     loss: 3.4371\n",
      "Epoch:   0 29.5%     loss: 3.4062\n",
      "Epoch:   0 30.3%     loss: 3.3890\n",
      "Epoch:   0 31.1%     loss: 3.4063\n",
      "Epoch:   0 31.8%     loss: 3.4113\n",
      "Epoch:   0 32.6%     loss: 3.4134\n",
      "Epoch:   0 33.4%     loss: 3.3973\n",
      "Epoch:   0 34.2%     loss: 3.3781\n",
      "Epoch:   0 35.0%     loss: 3.3841\n",
      "Epoch:   0 35.8%     loss: 3.3896\n",
      "Epoch:   0 36.6%     loss: 3.3646\n",
      "Epoch:   0 37.4%     loss: 3.3650\n",
      "Epoch:   0 38.2%     loss: 3.3474\n",
      "Epoch:   0 39.0%     loss: 3.3587\n",
      "Epoch:   0 39.8%     loss: 3.3592\n",
      "Epoch:   0 40.6%     loss: 3.3381\n",
      "Epoch:   0 41.4%     loss: 3.3572\n",
      "Epoch:   0 42.2%     loss: 3.3538\n",
      "Epoch:   0 43.0%     loss: 3.3504\n",
      "Epoch:   0 43.8%     loss: 3.3505\n",
      "Epoch:   0 44.6%     loss: 3.3517\n",
      "Epoch:   0 45.4%     loss: 3.3501\n",
      "Epoch:   0 46.2%     loss: 3.3213\n",
      "Epoch:   0 47.0%     loss: 3.3318\n",
      "Epoch:   0 47.8%     loss: 3.3317\n",
      "Epoch:   0 48.6%     loss: 3.3238\n",
      "Epoch:   0 49.4%     loss: 3.3093\n",
      "Epoch:   0 50.2%     loss: 3.3189\n",
      "Epoch:   0 51.0%     loss: 3.3151\n",
      "Epoch:   0 51.8%     loss: 3.3131\n",
      "Epoch:   0 52.5%     loss: 3.3012\n",
      "Epoch:   0 53.3%     loss: 3.2985\n",
      "Epoch:   0 54.1%     loss: 3.3124\n",
      "Epoch:   0 54.9%     loss: 3.2885\n",
      "Epoch:   0 55.7%     loss: 3.2664\n",
      "Epoch:   0 56.5%     loss: 3.2818\n",
      "Epoch:   0 57.3%     loss: 3.2682\n",
      "Epoch:   0 58.1%     loss: 3.2719\n",
      "Epoch:   0 58.9%     loss: 3.2584\n",
      "Epoch:   0 59.7%     loss: 3.2402\n",
      "Epoch:   0 60.5%     loss: 3.2628\n",
      "Epoch:   0 61.3%     loss: 3.2478\n",
      "Epoch:   0 62.1%     loss: 3.2597\n",
      "Epoch:   0 62.9%     loss: 3.2345\n",
      "Epoch:   0 63.7%     loss: 3.2348\n",
      "Epoch:   0 64.5%     loss: 3.2311\n",
      "Epoch:   0 65.3%     loss: 3.2205\n",
      "Epoch:   0 66.1%     loss: 3.2185\n",
      "Epoch:   0 66.9%     loss: 3.2148\n",
      "Epoch:   0 67.7%     loss: 3.2184\n",
      "Epoch:   0 68.5%     loss: 3.2063\n",
      "Epoch:   0 69.3%     loss: 3.2152\n",
      "Epoch:   0 70.1%     loss: 3.1855\n",
      "Epoch:   0 70.9%     loss: 3.1962\n",
      "Epoch:   0 71.7%     loss: 3.1796\n",
      "Epoch:   0 72.5%     loss: 3.1862\n",
      "Epoch:   0 73.2%     loss: 3.1775\n",
      "Epoch:   0 74.0%     loss: 3.1774\n",
      "Epoch:   0 74.8%     loss: 3.1599\n",
      "Epoch:   0 75.6%     loss: 3.1556\n",
      "Epoch:   0 76.4%     loss: 3.1620\n",
      "Epoch:   0 77.2%     loss: 3.1584\n",
      "Epoch:   0 78.0%     loss: 3.1394\n",
      "Epoch:   0 78.8%     loss: 3.1248\n",
      "Epoch:   0 79.6%     loss: 3.1330\n",
      "Epoch:   0 80.4%     loss: 3.1197\n",
      "Epoch:   0 81.2%     loss: 3.1117\n",
      "Epoch:   0 82.0%     loss: 3.1181\n",
      "Epoch:   0 82.8%     loss: 3.1128\n",
      "Epoch:   0 83.6%     loss: 3.0958\n",
      "Epoch:   0 84.4%     loss: 3.0828\n",
      "Epoch:   0 85.2%     loss: 3.0698\n",
      "Epoch:   0 86.0%     loss: 3.0655\n",
      "Epoch:   0 86.8%     loss: 3.0709\n",
      "Epoch:   0 87.6%     loss: 3.0739\n",
      "Epoch:   0 88.4%     loss: 3.0623\n",
      "Epoch:   0 89.2%     loss: 3.0615\n",
      "Epoch:   0 90.0%     loss: 3.0567\n",
      "Epoch:   0 90.8%     loss: 3.0478\n",
      "Epoch:   0 91.6%     loss: 3.0310\n",
      "Epoch:   0 92.4%     loss: 3.0304\n",
      "Epoch:   0 93.2%     loss: 3.0449\n",
      "Epoch:   0 93.9%     loss: 3.0349\n",
      "Epoch:   0 94.7%     loss: 3.0295\n",
      "Epoch:   0 95.5%     loss: 3.0113\n",
      "Epoch:   0 96.3%     loss: 3.0104\n",
      "Epoch:   0 97.1%     loss: 2.9976\n",
      "Epoch:   0 97.9%     loss: 3.0044\n",
      "Epoch:   0 98.7%     loss: 3.0029\n",
      "Epoch:   0 99.5%     loss: 3.0022\n",
      "6.902110815048218\n"
     ]
    }
   ],
   "source": [
    "trace = {'tloss': []}\n",
    "\n",
    "ts = time.time()\n",
    "for e in range(1):\n",
    "    \n",
    "    ### Train ###\n",
    "    model.train()\n",
    "    hidden = None  # reset LSTM hidden state\n",
    "    for i in range(0, train_x.shape[1]-1, n_seq):\n",
    "        \n",
    "        # Pick mini-batch (over seqence dimension)\n",
    "        inputs = train_x[:,i:i+n_seq]        # shape [n_batch, n_seq], less for last batch\n",
    "        targets = train_x[:,i+1:i+1+n_seq]   # shape [n_batch, n_seq], less for last batch\n",
    "        if inputs.shape[1] != targets.shape[1]:\n",
    "            inputs = inputs[:,:-1]           # fix shape disparity for last batch in epoch\n",
    "        \n",
    "        assert inputs.shape == targets.shape\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        hidden = tuple(h.detach() for h in hidden)\n",
    "        loss = criterion(outputs.view(-1, n_out), targets.flatten())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record per-iteration loss\n",
    "        trace['tloss'].append( loss.item() )\n",
    "        \n",
    "        print(f'Epoch: {e:3} {i*100/train_x.shape[1]:4.1f}%     loss: {loss.item():.4f}')\n",
    "        \n",
    "        \n",
    "\n",
    "print(time.time() - ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  8,  1, 16,  0, 23,  1,  0,  9,  2],\n",
       "        [ 6,  1,  0,  5,  9,  6,  0,  5,  4,  2],\n",
       "        [ 0, 15,  3, 13, 45, 10, 10, 53,  3, 11]], device='cuda:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8,  1, 16,  0, 23,  1,  0,  9,  2,  0],\n",
       "        [ 1,  0,  5,  9,  6,  0,  5,  4,  2,  8],\n",
       "        [15,  3, 13, 45, 10, 10, 53,  3, 11,  6]], device='cuda:0')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[:3, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo = outputs.view(-1, n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Call .contiguous() before .view(). at /opt/conda/conda-bld/pytorch_1544176307774/work/aten/src/THC/generic/THCTensor.cpp:220",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-0225108e5a10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Call .contiguous() before .view(). at /opt/conda/conda-bld/pytorch_1544176307774/work/aten/src/THC/generic/THCTensor.cpp:220"
     ]
    }
   ],
   "source": [
    "targets.view(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = targets.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.2124, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(oo, tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([55, 48,  7, 30, 35, 62, 55, 30, 55, 60,  4, 36,  0, 59, 11,  4, 46, 27,\n",
       "        48,  4, 35,  9,  4, 35,  5, 48, 27, 31,  4,  4, 42, 35, 35, 12, 27,  2,\n",
       "         4, 35, 30, 49, 38, 35, 42, 44, 51, 22, 49, 30,  4, 48, 33, 59,  1, 36,\n",
       "        42, 35, 43,  4, 44, 36, 35, 30, 27, 35, 64,  7, 35, 30, 27, 43, 40,  7,\n",
       "        39, 59, 59, 62,  1, 57, 23, 34, 54, 26,  0, 59, 66, 35, 49,  4, 44,  2,\n",
       "         4, 36,  7, 16, 35, 66, 35,  7,  5,  4,  4, 31, 49, 12,  4,  7,  7, 35,\n",
       "        42, 55, 44, 12,  4, 31, 30, 33, 59, 52, 51, 31, 49, 35, 44,  7, 35, 43,\n",
       "        27,  2, 35, 46, 44, 12, 12,  7, 59, 23,  5, 27, 36, 35, 38, 27, 51, 48,\n",
       "        35, 49,  4, 44, 42,  7, 61, 61, 55,  7, 35, 36, 27, 30, 49, 55, 55, 36,\n",
       "        42,  4,  4, 42, 33, 35, 30,  4, 12, 12, 35, 43,  4, 35, 49, 27,  9, 35,\n",
       "        43, 51, 31, 49, 39, 59, 59, 10,  1, 21, 37, 35, 35,  9,  4, 12, 12, 35,\n",
       "        48,  4,  5, 51, 30,  4, 42, 39, 59, 59, 63, 23, 37, 54,  0, 59, 53, 44,\n",
       "        30, 49, 35, 49,  4, 35, 36, 27, 54, 34, 34, 54,  0, 59, 21,  4, 55, 22,\n",
       "        36, 55,  4, 48, 33, 35, 55,  7, 64, 30, 35, 30, 49, 27, 51, 35, 30, 49,\n",
       "        44, 30, 35, 30, 21, 18,  1,  3,  1,  0, 59, 26, 49,  4, 35, 22, 27, 42,\n",
       "         7, 35, 46, 27, 48, 40, 55, 42, 35,  4, 12,  7,  4, 16, 59, 59, 47, 55,\n",
       "         7, 35, 49, 44,  7, 30,  4, 59, 62, 48, 44, 43,  7, 35, 49, 55,  7, 35,\n",
       "        48, 55, 31, 49, 35, 30, 49, 55,  4,  2,  4, 48, 38, 35, 30, 35, 36, 27,\n",
       "        30, 35,  4, 36, 27, 51, 22, 49, 33, 35, 55,  7, 64, 30, 35, 36, 27, 30,\n",
       "        35,  4, 36, 27, 51, 22, 49, 33, 35, 38,  4, 33, 59, 63,  4, 12, 55, 22,\n",
       "        49, 30,  4, 42, 35, 30, 49,  4, 43, 35, 55, 36, 35, 44, 36, 38, 35, 27,\n",
       "        30, 49,  4, 48, 35, 46, 49,  4, 48, 35, 31, 12, 27,  7,  4, 50, 35, 22,\n",
       "        55,  2,  4, 35, 49,  4, 48, 35, 22, 27, 27, 42, 35,  9, 44, 30, 31, 49,\n",
       "        33, 59, 35,  7, 27, 35, 51,  5,  9, 44, 48, 42, 35, 44, 36, 42, 59, 51,\n",
       "         5,  9, 44, 48, 42, 33, 35, 44, 36, 42, 35, 44, 12, 12, 35,  9,  4, 43,\n",
       "        27, 36, 35, 42, 55, 42, 35, 43, 38, 35, 12, 44, 36, 42, 35,  4,  6, 30,\n",
       "         4, 36, 42, 39, 59, 59, 45, 34,  1, 56, 18, 23, 27, 35, 49, 44,  2,  4,\n",
       "        35, 30, 49,  4, 35, 36, 27, 40, 12,  4, 35, 42, 51,  8,  4, 35, 44, 12,\n",
       "        55,  2,  4, 39, 59, 47, 49, 44], device='cuda:0')"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 35,  9, 55, 30, 49, 35, 43, 38, 35,  7, 27, 51, 12, 35,  0],\n",
       "        [36, 59, 26, 49,  4,  7,  4, 35, 42,  4, 44, 42, 35, 43,  4,  0],\n",
       "        [ 4, 35, 55,  7, 35, 44, 35,  5, 48, 27, 36,  4, 35, 44, 36,  0],\n",
       "        [27, 12, 44, 30,  4, 35, 55,  7, 12,  4, 33, 35,  4, 12,  7,  0],\n",
       "        [ 0, 59, 18, 46, 35, 55, 30, 35, 40,  4, 35, 12, 27,  2,  4,  0],\n",
       "        [55, 30, 49, 27, 51, 30, 35, 42,  4,  7,  4, 48, 30, 35,  7,  0],\n",
       "        [30,  7, 41, 59, 59, 17, 66,  1,  3, 35, 34,  1, 35, 57, 23,  0],\n",
       "        [ 4,  7, 30, 35, 42,  4, 46,  4, 36, 31,  4, 39, 59, 59, 10,  0],\n",
       "        [ 9, 35,  9, 55, 30, 49, 35, 44, 35, 48, 27, 40, 40,  4, 48,  0],\n",
       "        [35, 30, 49, 55,  7, 35,  7, 31, 27, 48, 36, 41, 59, 18,  7,  0],\n",
       "        [44, 48,  7, 33, 35, 27, 48, 35, 44, 36, 38, 35,  7,  4, 36,  0],\n",
       "        [34,  1, 23, 63, 18, 23, 52,  0, 59, 45, 27, 12, 12, 27,  9,  0],\n",
       "        [ 7, 35, 44, 36, 38, 35,  7, 30, 27, 36,  4, 33, 35, 44, 36,  0],\n",
       "        [26, 18, 10, 66,  3,  0, 59, 26, 27, 35, 34, 44, 31,  4, 42,  0],\n",
       "        [ 7, 55, 22, 49,  7, 33, 59,  1, 36, 42, 35, 44, 12, 12, 35,  0],\n",
       "        [35, 49,  4, 48, 35,  5, 48, 55, 36, 31,  4,  7, 35, 44, 48,  0]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
