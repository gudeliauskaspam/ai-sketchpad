{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents **Sequence-to-Sequence** encoder-decoder achitecture based on **LSTM** cells. Neural network is used to learn **English to French** translation task on a small corpus of sequences (."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/seq2seq.png\"/>\n",
    "<center>Sequence to sequence architecuter</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "* [Udacity NLP Nanodegree](https://eu.udacity.com/course/natural-language-processing-nanodegree--nd892) - I found dataset as part of the course\n",
    "* [Udacity NLP GitHub](https://github.com/udacity/aind2-nlp-capstone) - dataset link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code**\n",
    "\n",
    "* [A ten-minute introduction to sequence-to-sequence learning in Keras](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) - code with explanation\n",
    "* [Official Keras Seq2Seq Example](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py) - code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "\n",
    "* [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215) (2014) by Ilya Sutskever, Oriol Vinyals, Quoc V. Le\n",
    "* [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078) (2014) by Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit TensorFlow GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)  # init TF ...\n",
    "config=tf.ConfigProto(gpu_options=gpu_options)  # w/o taking ...\n",
    "with tf.Session(config=config): pass            # all GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to French Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from the link in the introduction and point path below to folder with *small_vocab_en* and *small_vocab_fr*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = '/home/marcin/Udacity/NLPND/aind2-nlp-capstone/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*small_vocab_en* contains approx 137860 short sentences in English. *small_vocab_fr* contains corresponding sentences in french."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 137860\n",
      "example sentences:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ST new jersey is sometimes quiet during autumn , and it is snowy in april . EN',\n",
       " 'ST the united states is usually chilly during july , and it is usually freezing in november . EN',\n",
       " 'ST california is usually quiet during march , and it is usually hot in june . EN']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(dataset_location, 'small_vocab_en')) as f:\n",
    "    # line below: 1) reads lines from file, 2) strips /n char and converts to lowercase, 3) adds special start/end words\n",
    "    data_en_raw = list(map(lambda x: 'ST '+x.strip().lower()+' EN', f.readlines()))\n",
    "print('len:', len(data_en_raw))\n",
    "print('example sentences:')\n",
    "data_en_raw[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 137860\n",
      "example sentences:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"ST new jersey est parfois calme pendant l' automne , et il est neigeux en avril . EN\",\n",
       " 'ST les états-unis est généralement froid en juillet , et il gèle habituellement en novembre . EN',\n",
       " 'ST california est généralement calme en mars , et il est généralement chaud en juin . EN']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(dataset_location, 'small_vocab_fr')) as f:\n",
    "    # line below: 1) reads lines from file, 2) strips /n char and converts to lowercase, 3) adds special start/end words\n",
    "    data_fr_raw = list(map(lambda x: 'ST '+x.strip().lower()+' EN', f.readlines()))\n",
    "print('len:', len(data_fr_raw))\n",
    "print('example sentences:')\n",
    "data_fr_raw[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Keras tokenizer to convert text sentences to tokens. Each word gets it's own unique integer token. Special words ST/EN also get their tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 19, 25, 1, 10, 69, 6, 41, 9, 5, 1, 57, 4, 46, 3], [2, 7, 22, 23, 1, 11, 64, 6, 45, 9, 5, 1, 11, 53, 4, 47, 3], [2, 24, 1, 11, 69, 6, 40, 9, 5, 1, 11, 70, 4, 36, 3]]\n"
     ]
    }
   ],
   "source": [
    "tok_en = tf.keras.preprocessing.text.Tokenizer(lower=False)\n",
    "tok_en.fit_on_texts(data_en_raw)\n",
    "data_en_tok = tok_en.texts_to_sequences(data_en_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example tokens for English:\n",
      "is: 1     ST: 2     EN: 3     in: 4     it: 5\n",
      "example sentences after tokenization:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 19, 25, 1, 10, 69, 6, 41, 9, 5, 1, 57, 4, 46, 3],\n",
       " [2, 7, 22, 23, 1, 11, 64, 6, 45, 9, 5, 1, 11, 53, 4, 47, 3],\n",
       " [2, 24, 1, 11, 69, 6, 40, 9, 5, 1, 11, 70, 4, 36, 3]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('example tokens for English:')\n",
    "print('is:', tok_en.word_index['is'], '   ',\n",
    "      'ST:', tok_en.word_index['ST'], '   ',\n",
    "      'EN:', tok_en.word_index['EN'], '   ',\n",
    "      'in:', tok_en.word_index['in'], '   ',\n",
    "      'it:', tok_en.word_index['it'])\n",
    "print('example sentences after tokenization:')\n",
    "data_en_tok[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_fr = tf.keras.preprocessing.text.Tokenizer(lower=False)\n",
    "tok_fr.fit_on_texts(data_fr_raw)\n",
    "data_fr_tok = tok_fr.texts_to_sequences(data_fr_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example tokens for French:\n",
      "est: 1     ST: 2     EN: 3     en: 4     il: 5\n",
      "example sentences after tokenization:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 37, 36, 1, 10, 69, 39, 13, 26, 8, 5, 1, 114, 4, 52, 3],\n",
       " [2, 6, 34, 33, 1, 14, 21, 4, 51, 8, 5, 97, 71, 4, 53, 3],\n",
       " [2, 103, 1, 14, 69, 4, 47, 8, 5, 1, 14, 23, 4, 43, 3]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('example tokens for French:')\n",
    "print('est:', tok_fr.word_index['est'], '   ',\n",
    "      'ST:', tok_fr.word_index['ST'], '   ',\n",
    "      'EN:', tok_fr.word_index['EN'], '   ',\n",
    "      'en:', tok_fr.word_index['en'], '   ',\n",
    "      'il:', tok_fr.word_index['il'])\n",
    "print('example sentences after tokenization:')\n",
    "data_fr_tok[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in either English or French: 23 tokens (including EN/ST)\n"
     ]
    }
   ],
   "source": [
    "max_len_en = len(max(data_en_tok, key=len))\n",
    "max_len_fr = len(max(data_fr_tok, key=len))\n",
    "max_len_both = max(max_len_en, max_len_fr)\n",
    "print('Maximum sentence length in either English or French:', max_len_both, 'tokens (including EN/ST)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad both corpuses to longest sentence in each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en_tok, maxlen=max_len_en, padding='post')\n",
    "data_fr = tf.keras.preprocessing.sequence.pad_sequences(data_fr_tok, maxlen=max_len_fr, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length English sentence (tokens):    17\n",
      "Max length French sentence (tokens):     23\n",
      "Num tokens in English vocabulary:        201\n",
      "Num tokens in English vocabulary:        346\n"
     ]
    }
   ],
   "source": [
    "n_en_seq = data_en.shape[1]\n",
    "n_fr_seq = data_fr.shape[1]\n",
    "n_en_vocab = len(tok_en.word_index)\n",
    "n_fr_vocab = len(tok_fr.word_index)\n",
    "max_seq_len = max(n_en_seq, n_fr_seq)\n",
    "print('Max length English sentence (tokens):   ', n_en_seq)\n",
    "print('Max length French sentence (tokens):    ', n_fr_seq)\n",
    "print('Num tokens in English vocabulary:       ', n_en_vocab)\n",
    "print('Num tokens in English vocabulary:       ', n_fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English train data\n",
      "shape: (137860, 17)\n",
      "[[ 2 31 14 18 15  1  7 84  8 32 14 18  1  7 85  3  0]\n",
      " [ 2 33 13 15  1  7 86  8 32 13  1  7 84  3  0  0  0]\n",
      " [ 2 20  1 68  6 49  8  5  1 11 64  4 45  3  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print('English train data')\n",
    "print('shape:', data_en.shape)\n",
    "print(data_en[4:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French train targets data\n",
      "shape: (137860, 23)\n",
      "[[ 2 42 15 16 18  1 12 84  7 41 15 16  1  9 85  3  0  0  0  0  0  0  0]\n",
      " [ 2 22 18 19  1 86  7 41 19  1 12 84  3  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 31  1 60  4 54  7  5  1 14 21  4 51  3  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print('French train targets data')\n",
    "print('shape:', data_fr.shape)\n",
    "print(data_fr[4:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/rnn_bidirectional.png\"/>\n",
    "<center>Figure from Bidirectional Recurrent Neural Networks (1997) by Mike Schuster and kuldip K. Paliwal</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 23, 50)            10050     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 23, 128)           44160     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 23, 346)           44634     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 23, 346)           0         \n",
      "=================================================================\n",
      "Total params: 98,844\n",
      "Trainable params: 98,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, GRU, TimeDistributed, Dense, Activation\n",
    "\n",
    "X_input = Input(shape=(n_en_seq,))\n",
    "X = Embedding(input_dim=n_en_vocab, output_dim=50)(X_input)\n",
    "X = Bidirectional( GRU(units=64, return_sequences=True) )(X)\n",
    "X = TimeDistributed(Dense(units=n_fr_vocab))(X)\n",
    "X = Activation('softmax')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=X_input, outputs=X)\n",
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcin/.anaconda/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27572 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 10s 92us/step - loss: 3.5588 - sparse_categorical_accuracy: 0.4012 - val_loss: nan - val_sparse_categorical_accuracy: 0.4732\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 9s 78us/step - loss: 2.3705 - sparse_categorical_accuracy: 0.4941 - val_loss: nan - val_sparse_categorical_accuracy: 0.5431\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 9s 77us/step - loss: 1.6129 - sparse_categorical_accuracy: 0.6072 - val_loss: nan - val_sparse_categorical_accuracy: 0.6707\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 9s 77us/step - loss: 1.1627 - sparse_categorical_accuracy: 0.7082 - val_loss: nan - val_sparse_categorical_accuracy: 0.7400\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 9s 78us/step - loss: 0.9342 - sparse_categorical_accuracy: 0.7557 - val_loss: nan - val_sparse_categorical_accuracy: 0.7703\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 8s 77us/step - loss: 0.7899 - sparse_categorical_accuracy: 0.7832 - val_loss: nan - val_sparse_categorical_accuracy: 0.7943\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 9s 78us/step - loss: 0.6844 - sparse_categorical_accuracy: 0.8059 - val_loss: nan - val_sparse_categorical_accuracy: 0.8167\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 9s 78us/step - loss: 0.6039 - sparse_categorical_accuracy: 0.8247 - val_loss: nan - val_sparse_categorical_accuracy: 0.8335\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 9s 79us/step - loss: 0.5409 - sparse_categorical_accuracy: 0.8410 - val_loss: nan - val_sparse_categorical_accuracy: 0.8489\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 9s 79us/step - loss: 0.4906 - sparse_categorical_accuracy: 0.8543 - val_loss: nan - val_sparse_categorical_accuracy: 0.8605\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=data_en, y=np.expand_dims(data_fr, axis=-1),\n",
    "                 batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_english(seq):\n",
    "    words = [tok_en.index_word[x] for x in seq if x in tok_en.index_word]\n",
    "    return ' '.join(words)\n",
    "def sequence_to_french(seq):\n",
    "    words = [tok_fr.index_word[x] for x in seq if x in tok_fr.index_word]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english:             ST we dislike oranges , grapefruit , and bananas . EN\n",
      "french (original):   ST nous détestons les oranges , le pamplemousse et les bananes . EN\n",
      "french (predicted):  ST nous détestons les le le pamplemousse et les les EN\n"
     ]
    }
   ],
   "source": [
    "index = 234\n",
    "english_sentence = data_en_raw[index]\n",
    "french_sentence = data_fr_raw[index]\n",
    "\n",
    "prediction_prob = model.predict(data_en[index:index+1])\n",
    "prediction_prob = prediction_prob.squeeze()\n",
    "prediction_tok = prediction_prob.argmax(axis=-1)\n",
    "predicted_sentence = sequence_to_french(prediction_tok)\n",
    "\n",
    "print('english:            ', english_sentence)\n",
    "print('french (original):  ', french_sentence)\n",
    "print('french (predicted): ', predicted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use technique called 'Teacher Forces' to train decoder. I.e. instead of getting decoder to generate one word at a time and then feed it into the next step in decoder, we will pretend decoder generated correct sequence and just feed in correct inputs. Because we know correct french translation, we don't have to sample one-at-a-time.\n",
    "\n",
    "To do this we will need two version of French dataset:\n",
    "\n",
    "* actual target dataset, with ST marker removed\n",
    "* feed-in target dataset, which we will use as input to decoder, this one contains ST token at first position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French train targets data\n",
      "shape: (137860, 23)\n",
      "[[42 15 16 18  1 12 84  7 41 15 16  1  9 85  3  0  0  0  0  0  0  0  0]\n",
      " [22 18 19  1 86  7 41 19  1 12 84  3  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [31  1 60  4 54  7  5  1 14 21  4 51  3  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "data_fr_noST = np.roll(data_fr, shift=-1, axis=-1)  # shift left by one and pad 0 on the right\n",
    "data_fr_noST[:,-1] = 0\n",
    "print('French train targets data')\n",
    "print('shape:', data_fr_noST.shape)\n",
    "print(data_fr_noST[4:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create following parts of graph:\n",
    "* Encoder\n",
    "  * inputs: whole English sentence\n",
    "  * outputs: LSTM hidden states\n",
    "* Decoder in train mode\n",
    "  * inputs: LSTM hidden states **and** target French sentence in \"teacher forcing\" mode (w/o ST token at the begining)\n",
    "  * outputs: whole French sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Enc_Input (InputLayer)          (None, 17)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Dec_Target (InputLayer)         (None, 23)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Enc_Embbeding (Embedding)       (None, 17, 50)       10050       Enc_Input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dec_Embbedingg (Embedding)      (None, 23, 50)       17300       Dec_Target[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "Enc_LSTM (LSTM)                 [(None, 512), (None, 1153024     Enc_Embbeding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Dec_LSTM (LSTM)                 [(None, 23, 512), (N 1153024     Dec_Embbedingg[0][0]             \n",
      "                                                                 Enc_LSTM[0][1]                   \n",
      "                                                                 Enc_LSTM[0][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "Dec_Output (Dense)              (None, 23, 346)      177498      Dec_LSTM[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,510,896\n",
      "Trainable params: 2,510,896\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Activation\n",
    "\n",
    "# Encoder\n",
    "E_input = Input(shape=(n_en_seq,), name='Enc_Input')                                       # encoder input tensor\n",
    "E = Embedding(input_dim=n_en_vocab, output_dim=50, name='Enc_Embbeding')(E_input)          # encoder embedding layer\n",
    "_, Eh, Ec = LSTM(units=512, return_state=True, name='Enc_LSTM')(E)                         # encoder LSTM layer\n",
    "\n",
    "# Decoder layer definitions\n",
    "decoder_embedding = Embedding(input_dim=n_fr_vocab, output_dim=50, name='Dec_Embbedingg')  # we will need to reuse these\n",
    "decoder_lstm = LSTM(512, return_sequences=True, return_state=True, name='Dec_LSTM')        # layers in sampling mode\n",
    "decoder_dense = Dense(n_fr_vocab, activation='softmax', name='Dec_Output')                 # in next section\n",
    "\n",
    "# Decoder in train mode\n",
    "D_input = Input(shape=(n_fr_seq,), name='Dec_Target')                                      # decoder input tensor\n",
    "D = decoder_embedding(D_input)                                                             # decoder embedding layer\n",
    "D, _, _ = decoder_lstm(D, initial_state=[Eh, Ec])                                          # decoder LSTM layer\n",
    "D_output = decoder_dense(D)                                                                # decoder dense on output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create end-to-end Keras model for training. Contains both encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=[E_input, D_input], outputs=D_output)                        # full seq-2-seq model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcin/.anaconda/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27572 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 24s 219us/step - loss: 2.8013 - sparse_categorical_accuracy: 0.4601 - val_loss: nan - val_sparse_categorical_accuracy: 0.5576\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 22s 203us/step - loss: 1.6133 - sparse_categorical_accuracy: 0.6177 - val_loss: nan - val_sparse_categorical_accuracy: 0.6952\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 22s 204us/step - loss: 0.8836 - sparse_categorical_accuracy: 0.7342 - val_loss: nan - val_sparse_categorical_accuracy: 0.7608\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 22s 203us/step - loss: 0.7734 - sparse_categorical_accuracy: 0.7541 - val_loss: nan - val_sparse_categorical_accuracy: 0.7796\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 23s 205us/step - loss: 0.6238 - sparse_categorical_accuracy: 0.7892 - val_loss: nan - val_sparse_categorical_accuracy: 0.7961\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 23s 209us/step - loss: 0.5685 - sparse_categorical_accuracy: 0.8029 - val_loss: nan - val_sparse_categorical_accuracy: 0.8112\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 23s 205us/step - loss: 0.5298 - sparse_categorical_accuracy: 0.8162 - val_loss: nan - val_sparse_categorical_accuracy: 0.8272\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 23s 204us/step - loss: 0.4960 - sparse_categorical_accuracy: 0.8285 - val_loss: nan - val_sparse_categorical_accuracy: 0.8297\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 23s 205us/step - loss: 0.4707 - sparse_categorical_accuracy: 0.8369 - val_loss: nan - val_sparse_categorical_accuracy: 0.8408\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 23s 207us/step - loss: 0.4503 - sparse_categorical_accuracy: 0.8434 - val_loss: nan - val_sparse_categorical_accuracy: 0.8447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe65a593f28>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[data_en, data_fr], y=np.expand_dims(data_fr_noST, axis=-1),\n",
    "          batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model for sampling translations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Keras model for encoder as separate unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'E_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-ef398c58424b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mE_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'E_input' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = tf.keras.Model(inputs=E_input, outputs=[Eh, Ec])\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create decoder in sampling mode, reuse layer definitions form previous section\n",
    "* Inputs: hidden states **and** single English word-token (not whole French sentence)\n",
    "* Outputs: single French word-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sh_init = Input(shape=(512,))\n",
    "Sc_init = Input(shape=(512,))\n",
    "S_input = Input(shape=(1,), name='Sam_Input')\n",
    "S = decoder_embedding(S_input)\n",
    "S, Sh, Sc = decoder_lstm(S, initial_state=[Sh_init, Sc_init])\n",
    "S_output = decoder_dense(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Keras model for decoder-sampler (one word at a time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = tf.keras.Model(inputs=[S_input, Sh_init, Sc_init], outputs=[S_output, Sh, Sc])\n",
    "sampler.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english:             ST his least favorite fruit is the pear , but our least favorite is the banana . EN\n",
      "french (original):   ST son fruit préféré est moins la poire , mais notre moins préféré est la banane . EN\n"
     ]
    }
   ],
   "source": [
    "index = 666\n",
    "english_sentence = data_en_raw[index]\n",
    "french_sentence = data_fr_raw[index]\n",
    "print('english:            ', english_sentence)\n",
    "print('french (original):  ', french_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Actually Sample**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run input sentence through encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_h, st_c = encoder.predict(data_en[index:index+1])\n",
    "assert st_h.shape == (1, 512) and st_c.shape == (1, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input variables - thse will be feed into decoder at first decode time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_input = tok_fr.word_index['ST']\n",
    "st_input = np.array([[st_input]])  # batch size = 1, seq len = 1\n",
    "assert st_input.shape == (1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate output words one-at-a-time and feed them back next time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tok = []                                              # list of output tokens, generated one at a time\n",
    "for i in range(n_fr_seq):\n",
    "    # feed one word (st_input) intot decoder\n",
    "    probs, st_h, st_c = sampler.predict([st_input, st_h, st_c])\n",
    "    assert st_h.shape == (1, 512) and st_c.shape == (1, 512)\n",
    "    \n",
    "    # pick maximum probability prediction as next word\n",
    "    # (but keep shape so we can feed in next step)\n",
    "    st_input = probs.argmax(axis=-1)\n",
    "    assert st_input.shape == (1, 1)\n",
    "    \n",
    "    # pick maximum probability prediction and append to generate list\n",
    "    # (this does same as line above, but discards shape)\n",
    "    token = probs.argmax()\n",
    "    prediction_tok.append(token)\n",
    "    \n",
    "    # if decoder generated special end-word, break\n",
    "    if token == tok_fr.word_index['EN']:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print output sentence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 18, 19, 1, 15, 9, 90, 7, 22, 15, 19, 1, 9, 91, 3]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print input english sentence, target French and generated French sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english:             ST his least favorite fruit is the pear , but our least favorite is the banana . EN\n",
      "french (original):   ST son fruit préféré est moins la poire , mais notre moins préféré est la banane . EN\n",
      "french (predicted):  son fruit préféré est moins la poire mais son moins préféré est la fraise EN\n"
     ]
    }
   ],
   "source": [
    "print('english:            ', english_sentence)\n",
    "print('french (original):  ', french_sentence)\n",
    "predicted_sentence = sequence_to_french(prediction_tok)\n",
    "print('french (predicted): ', predicted_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
