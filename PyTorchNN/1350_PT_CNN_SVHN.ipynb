{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks presents **ConvNet** in PyTorch used to solve **Street View House Numbers** task.\n",
    "\n",
    "This is replication of _Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks_\n",
    "\n",
    "**Contents**\n",
    "* [Imports](#Imports)\n",
    "* [Dataset](#Dataset)\n",
    "* [Model](#Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and extract [SVHN](http://ufldl.stanford.edu/housenumbers/) dataset in **Format 1** (train.tar.gz, test.tar.gz, extra.tar.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = '/home/marcin/Datasets/SVHN'  # .../train/1.png\n",
    "model_save_location = './models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pathlib\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py  # required to open .mat files in SVHN dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = pathlib.Path(dataset_location)\n",
    "assert os.path.isfile(dataset_path / 'extra/1.png')\n",
    "assert os.path.isfile(dataset_path / 'train/1.png')\n",
    "assert os.path.isfile(dataset_path / 'test/1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to read `.mat` files with labels and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_name(f, index):\n",
    "    \"\"\"Decode string from HDF5 file.\"\"\"\n",
    "    assert isinstance(f, h5py.File)\n",
    "    assert index == int(index)\n",
    "    ref = f['/digitStruct/name'][index][0]\n",
    "    return ''.join(chr(v[0]) for v in f[ref])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_digits_raw(f, index):\n",
    "    \"\"\"Decode digits and bounding boxes from HDF5 file.\"\"\"\n",
    "    assert isinstance(f, h5py.File)\n",
    "    assert index == int(index)\n",
    "    \n",
    "    ref = f['/digitStruct/bbox'][index].item()\n",
    "    ddd = {}\n",
    "    for key in ['label', 'left', 'top', 'width', 'height']:\n",
    "        dset = f[ref][key]\n",
    "        if len(dset) == 1:\n",
    "            ddd[key] = [ int(dset[0][0]) ]\n",
    "        else:\n",
    "            ddd[key] = []\n",
    "            for i in range(len(dset)):\n",
    "                ref2 = dset[i][0]\n",
    "                ddd[key].append( int(f[ref2][0][0]) )\n",
    "    return ddd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(ddict):\n",
    "    \"\"\"Convert raw digit info into len-5 label and single bounding box\"\"\"\n",
    "    assert isinstance(ddict, dict)\n",
    "    \n",
    "    # construct proper label for NN training\n",
    "    # image '210' -> [3, 2, 1, 10, 0, 0]\n",
    "    #                 ^  ^  ^  ^   ^--^-- \"0, 0\" pad with '0' (no digit)\n",
    "    #                 |  ---------------- \"210\" house number, 0 encoded as 10\n",
    "    #                 ------------------- \"3\" is number of digits\n",
    "    label = ddict['label'].copy()\n",
    "    label = [len(label)] + label + [0]*(5-len(label))\n",
    "    \n",
    "    left = min(ddict['left'])\n",
    "    top = min(ddict['top'])\n",
    "    right = max(l+w for l, w in zip(ddict['left'], ddict['width']))\n",
    "    bottom = max(t+h for t, h in zip(ddict['top'], ddict['height']))\n",
    "    return tuple(label), (left, top, right, bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mat_file(filepath):\n",
    "    \"\"\"Open .mat file and read all the metadata.\"\"\"\n",
    "    assert isinstance(filepath, (str, pathlib.PosixPath))\n",
    "    \n",
    "    print(filepath)\n",
    "    \n",
    "    meta = {'names':[], 'labels':[], 'bboxes':[]}\n",
    "    with h5py.File(filepath) as f:\n",
    "        length = len(f['/digitStruct/name'])\n",
    "        for i in range(10): # length):\n",
    "            name = read_name(f, i)\n",
    "            ddict = read_digits_raw(f, i)\n",
    "            label, bbox = get_label(ddict)\n",
    "            meta['names'].append(name)\n",
    "            meta['labels'].append(label)\n",
    "            meta['bboxes'].append(bbox)\n",
    "            if i % 1000 == 0 or i == length-1:\n",
    "                print(f'{i:6d} / {length}')\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_or_generate(name):\n",
    "    \"\"\"Either load .pkl, or if doesn't exit generate it and open.\"\"\"\n",
    "    assert name in ('extra', 'test', 'train')\n",
    "    \n",
    "    fname = name+'.pkl'\n",
    "    if os.path.exists(dataset_path / fname):\n",
    "        with open(dataset_path / fname, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "            print(f'Loaded:{fname}')\n",
    "    else:\n",
    "        print(f'Generating {fname}:')\n",
    "        meta = read_mat_file(dataset_path / name / 'digitStruct.mat')\n",
    "        with open(dataset_path / fname, 'wb') as f:\n",
    "            pickle.dump(meta, f)\n",
    "    \n",
    "    return meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** this may take <u>around one hour</u> to complete, but only on the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded:extra.pkl\n",
      "Loaded:test.pkl\n",
      "Loaded:train.pkl\n"
     ]
    }
   ],
   "source": [
    "# Convert label/bbox data to friendly format\n",
    "extra_meta = open_or_generate('extra')\n",
    "test_meta = open_or_generate('test')\n",
    "train_meta = open_or_generate('train')\n",
    "\n",
    "# Add folder information\n",
    "extra_meta['folders'] = ['extra'] * len(extra_meta['names'])\n",
    "test_meta['folders'] = ['test'] * len(test_meta['names'])\n",
    "train_meta['folders'] = ['train'] * len(train_meta['names'])\n",
    "\n",
    "# Add 'extra' to 'train' data\n",
    "train_meta['names'].extend(extra_meta['names'])\n",
    "train_meta['labels'].extend(extra_meta['labels'])\n",
    "train_meta['bboxes'].extend(extra_meta['bboxes'])\n",
    "train_meta['folders'].extend(extra_meta['folders'])\n",
    "del extra_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVHNDataset: #(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, metadata, transforms=None):\n",
    "        assert isinstance(dataset_path, (str, pathlib.PosixPath))\n",
    "        assert isinstance(metadata, dict)\n",
    "        assert set(metadata.keys()) == {'bboxes','folders','labels','names'}\n",
    "        assert len(metadata['names']) == len(metadata['labels'])\n",
    "        assert len(metadata['names']) == len(metadata['bboxes'])\n",
    "        assert len(metadata['names']) == len(metadata['folders'])\n",
    "        assert transforms is None or \\\n",
    "               isinstance(transforms, torchvision.transforms.Compose)\n",
    "        \n",
    "        self.dataset_path = pathlib.PosixPath(dataset_path)\n",
    "        self.metadata = metadata\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata['names'])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.metadata['names'][index]      # e.g. '1.png'\n",
    "        image_folder = self.metadata['folders'][index]  # e.g. 'test'\n",
    "        label = self.metadata['labels'][index]  # [1, 2, 10, 0, 0]\n",
    "        bbox = self.metadata['bboxes'][index]   # [left, top, right, bottom]\n",
    "        \n",
    "        # Figure out crop box\n",
    "        left, top, right, bottom = bbox\n",
    "        width, height = right - left, bottom - top\n",
    "        crop_left =   int(round(left   - .15*width))\n",
    "        crop_top =    int(round(top    - .15*height))\n",
    "        crop_right =  int(round(right  + .15*width))\n",
    "        crop_bottom = int(round(bottom + .15*height))\n",
    "        \n",
    "        img = PIL.Image.open(self.dataset_path / image_folder / image_name)\n",
    "        img2 = img.crop(box=(crop_left, crop_top, crop_right, crop_bottom))\n",
    "        res = img2.resize((64, 64))\n",
    "        if self.transforms is not None:\n",
    "            res = self.transforms(res)\n",
    "        \n",
    "        return res, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create temporary dataset to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SVHNDataset(dataset_path, train_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: (3, 3, 3, 2, 0, 0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F1161827908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = dataset[200000]\n",
    "print(f'Label: {label}')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create actual datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomCrop([54, 54]),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "transforms_valid = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.CenterCrop([54, 54]),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "dataset_train = SVHNDataset(dataset_path, train_meta, transforms_train)\n",
    "dataset_valid = SVHNDataset(dataset_path, test_meta, transforms_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And dataloaders as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=6, pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset_valid, batch_size=128, shuffle=False,\n",
    "    num_workers=6, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVHNModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        def _block(in_channels, out_channels, stride):\n",
    "            \"\"\"Helper to build CNN blocks.\"\"\"\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                          kernel_size=5, padding=2),\n",
    "                nn.BatchNorm2d(num_features=out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=stride, padding=1),\n",
    "                nn.Dropout(0.2)\n",
    "            )\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.block1 = _block(in_channels=  3, out_channels= 48, stride=2)\n",
    "        self.block2 = _block(in_channels= 48, out_channels= 64, stride=1)\n",
    "        self.block3 = _block(in_channels= 64, out_channels=128, stride=2)\n",
    "        self.block4 = _block(in_channels=128, out_channels=160, stride=1)\n",
    "        self.block5 = _block(in_channels=160, out_channels=192, stride=2)\n",
    "        self.block6 = _block(in_channels=192, out_channels=192, stride=1)\n",
    "        self.block7 = _block(in_channels=192, out_channels=192, stride=2)\n",
    "        self.block8 = _block(in_channels=192, out_channels=192, stride=1)\n",
    "        self.fc1 = nn.Sequential(nn.Linear(192 * 7 * 7, 3072), nn.ReLU())\n",
    "        self.fc2 = nn.Sequential(nn.Linear(3072, 3072), nn.ReLU())\n",
    "        \n",
    "        self.length = nn.Sequential(nn.Linear(3072, 7))\n",
    "        self.digit1 = nn.Sequential(nn.Linear(3072, 11))\n",
    "        self.digit2 = nn.Sequential(nn.Linear(3072, 11))\n",
    "        self.digit3 = nn.Sequential(nn.Linear(3072, 11))\n",
    "        self.digit4 = nn.Sequential(nn.Linear(3072, 11))\n",
    "        self.digit5 = nn.Sequential(nn.Linear(3072, 11))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.block8(x)\n",
    "        x = x.view(x.size(0), 192*7*7)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        length = self.length(x)  #  logits!\n",
    "        digit1 = self.digit1(x)\n",
    "        digit2 = self.digit2(x)\n",
    "        digit3 = self.digit3(x)\n",
    "        digit4 = self.digit4(x)\n",
    "        digit5 = self.digit5(x)\n",
    "        \n",
    "        return length, digit1, digit2, digit3, digit4, digit5    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(logits, targets):\n",
    "    \"\"\"Custom loss function.\n",
    "    \n",
    "    Params:\n",
    "        logits (list): with following members:\n",
    "            logits[0] (torch.Tensor): length, shape [n_batch, 7], LOGITS!\n",
    "            logits[1] (torch.Tensor): digit1, shape [n_batch, 11], LOGITS!\n",
    "            ...\n",
    "        targets (list): with members:\n",
    "            targets[0] (torch.Tensor): length target, shape [n_batch]\n",
    "            targets[1] (torch.Tensor): digit1 target, shape [n_batch]\n",
    "            ...\n",
    "    \"\"\"\n",
    "    assert len(logits) == len(targets)\n",
    "    length_ce = F.cross_entropy(logits[0], targets[0])\n",
    "    digit1_ce = F.cross_entropy(logits[1], targets[1])\n",
    "    digit2_ce = F.cross_entropy(logits[2], targets[2])\n",
    "    digit3_ce = F.cross_entropy(logits[3], targets[3])\n",
    "    digit4_ce = F.cross_entropy(logits[4], targets[4])\n",
    "    digit5_ce = F.cross_entropy(logits[5], targets[5])\n",
    "    loss = length_ce + digit1_ce + digit2_ce + \\\n",
    "           digit3_ce + digit4_ce + digit5_ce\n",
    "    assert list(loss.size()) == []\n",
    "    return loss  # tensor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_acc_sum(logits, targets):\n",
    "    \"\"\"Custom accuracy function.\n",
    "    \n",
    "    Params:\n",
    "        same as custom_loss()\n",
    "    \"\"\"\n",
    "    assert len(logits) == len(targets)\n",
    "    \n",
    "    length_predictions = logits[0].max(dim=1)[1]\n",
    "    digit1_predictions = logits[1].max(dim=1)[1]\n",
    "    digit2_predictions = logits[2].max(dim=1)[1]\n",
    "    digit3_predictions = logits[3].max(dim=1)[1]\n",
    "    digit4_predictions = logits[4].max(dim=1)[1]\n",
    "    digit5_predictions = logits[5].max(dim=1)[1]\n",
    "    \n",
    "    accumulate = torch.ones_like(targets[0], dtype=torch.uint8)\n",
    "    accumulate &= length_predictions.eq(targets[0])\n",
    "    accumulate &= digit1_predictions.eq(targets[1])\n",
    "    accumulate &= digit2_predictions.eq(targets[2])\n",
    "    accumulate &= digit3_predictions.eq(targets[3])\n",
    "    accumulate &= digit4_predictions.eq(targets[4])\n",
    "    accumulate &= digit5_predictions.eq(targets[5])\n",
    "    \n",
    "    accuracy_sum = accumulate.sum()\n",
    "    return accuracy_sum  # tensor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get PyTorch device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVHNModel()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=10000, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    accuracy_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = images.to(device)\n",
    "            targets = [t.to(device) for t in targets]\n",
    "            \n",
    "            logits = model(images)\n",
    "            acc = custom_acc_sum(logits, targets)\n",
    "            accuracy_sum += acc.item()\n",
    "    accuracy = accuracy_sum / len(data_loader.dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(model_save_location, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 0...\n",
      "  step:     100   loss: 7.401640   lr: 0.010000   i/s:  854.09\n",
      "  step:     200   loss: 7.190207   lr: 0.010000   i/s: 1030.37\n",
      "  step:     300   loss: 6.371806   lr: 0.010000   i/s: 1066.78\n",
      "  step:     400   loss: 6.779932   lr: 0.010000   i/s: 1061.85\n",
      "  step:     500   loss: 7.264554   lr: 0.010000   i/s: 1069.39\n",
      "  step:     600   loss: 6.446972   lr: 0.010000   i/s: 1072.06\n",
      "  step:     700   loss: 6.449721   lr: 0.010000   i/s: 1022.47\n",
      "  step:     800   loss: 6.281842   lr: 0.010000   i/s: 1046.71\n",
      "  step:     900   loss: 5.938114   lr: 0.010000   i/s: 1032.28\n",
      "  step:    1000   loss: 5.916947   lr: 0.010000   i/s:  991.43\n",
      "Evaluating...\n",
      "  accuracy: 0.0694\n",
      "Training...\n",
      "  step:    1100   loss: 5.644208   lr: 0.010000   i/s:  388.32\n",
      "  step:    1200   loss: 5.459051   lr: 0.010000   i/s:  960.80\n",
      "  step:    1300   loss: 4.829540   lr: 0.010000   i/s:  996.09\n",
      "  step:    1400   loss: 5.078192   lr: 0.010000   i/s:  971.94\n",
      "  step:    1500   loss: 5.588068   lr: 0.010000   i/s: 1049.77\n",
      "  step:    1600   loss: 5.007460   lr: 0.010000   i/s: 1013.82\n",
      "  step:    1700   loss: 4.824934   lr: 0.010000   i/s: 1040.74\n",
      "  step:    1800   loss: 4.935504   lr: 0.010000   i/s: 1061.68\n",
      "  step:    1900   loss: 4.612381   lr: 0.010000   i/s:  992.53\n",
      "  step:    2000   loss: 3.305910   lr: 0.010000   i/s:  962.54\n",
      "Evaluating...\n",
      "  accuracy: 0.2704\n",
      "Training...\n",
      "  step:    2100   loss: 4.393053   lr: 0.010000   i/s:  422.83\n",
      "  step:    2200   loss: 4.240945   lr: 0.010000   i/s: 1002.94\n",
      "  step:    2300   loss: 2.937595   lr: 0.010000   i/s:  993.45\n",
      "  step:    2400   loss: 3.259427   lr: 0.010000   i/s: 1029.44\n",
      "  step:    2500   loss: 3.005823   lr: 0.010000   i/s:  998.62\n",
      "  step:    2600   loss: 2.166938   lr: 0.010000   i/s:  961.51\n",
      "  step:    2700   loss: 2.812975   lr: 0.010000   i/s:  987.19\n",
      "  step:    2800   loss: 2.467856   lr: 0.010000   i/s:  978.88\n",
      "  step:    2900   loss: 3.406239   lr: 0.010000   i/s:  983.28\n",
      "  step:    3000   loss: 2.745082   lr: 0.010000   i/s:  949.98\n",
      "Evaluating...\n",
      "  accuracy: 0.6435\n",
      "Training...\n",
      "  step:    3100   loss: 1.782341   lr: 0.010000   i/s:  443.00\n",
      "  step:    3200   loss: 1.559707   lr: 0.010000   i/s: 1037.79\n",
      "  step:    3300   loss: 2.777167   lr: 0.010000   i/s: 1023.67\n",
      "  step:    3400   loss: 1.807530   lr: 0.010000   i/s: 1000.98\n",
      "  step:    3500   loss: 1.169201   lr: 0.010000   i/s: 1035.72\n",
      "  step:    3600   loss: 1.313325   lr: 0.010000   i/s:  986.18\n",
      "  step:    3700   loss: 1.078747   lr: 0.010000   i/s:  990.44\n",
      "  step:    3800   loss: 0.775579   lr: 0.010000   i/s:  945.11\n",
      "  step:    3900   loss: 1.446685   lr: 0.010000   i/s: 1004.49\n",
      "  step:    4000   loss: 1.214472   lr: 0.010000   i/s:  984.77\n",
      "Evaluating...\n",
      "  accuracy: 0.7061\n",
      "Training...\n",
      "  step:    4100   loss: 1.145120   lr: 0.010000   i/s:  406.99\n",
      "  step:    4200   loss: 1.026030   lr: 0.010000   i/s: 1044.03\n",
      "  step:    4300   loss: 1.048183   lr: 0.010000   i/s: 1039.08\n",
      "  step:    4400   loss: 1.248956   lr: 0.010000   i/s:  988.14\n",
      "  step:    4500   loss: 1.577457   lr: 0.010000   i/s: 1046.55\n",
      "  step:    4600   loss: 1.753427   lr: 0.010000   i/s: 1046.21\n",
      "  step:    4700   loss: 2.271700   lr: 0.010000   i/s: 1035.67\n",
      "  step:    4800   loss: 1.322761   lr: 0.010000   i/s:  996.96\n",
      "  step:    4900   loss: 0.958066   lr: 0.010000   i/s: 1042.63\n",
      "  step:    5000   loss: 2.174776   lr: 0.010000   i/s:  985.36\n",
      "Evaluating...\n",
      "  accuracy: 0.7358\n",
      "Training...\n",
      "  step:    5100   loss: 1.030763   lr: 0.010000   i/s:  406.58\n",
      "  step:    5200   loss: 0.818460   lr: 0.010000   i/s:  964.99\n",
      "  step:    5300   loss: 0.705028   lr: 0.010000   i/s: 1034.38\n",
      "  step:    5400   loss: 0.547387   lr: 0.010000   i/s: 1012.53\n",
      "  step:    5500   loss: 0.883970   lr: 0.010000   i/s:  975.82\n",
      "  step:    5600   loss: 1.818056   lr: 0.010000   i/s: 1018.23\n",
      "  step:    5700   loss: 1.106099   lr: 0.010000   i/s:  990.36\n",
      "  step:    5800   loss: 0.550250   lr: 0.010000   i/s:  985.19\n",
      "  step:    5900   loss: 1.230123   lr: 0.010000   i/s: 1037.23\n",
      "  step:    6000   loss: 1.819298   lr: 0.010000   i/s: 1041.18\n",
      "Evaluating...\n",
      "  accuracy: 0.8320\n",
      "Training...\n",
      "  step:    6100   loss: 1.042244   lr: 0.010000   i/s:  426.67\n",
      "  step:    6200   loss: 0.863782   lr: 0.010000   i/s: 1024.50\n",
      "  step:    6300   loss: 0.883616   lr: 0.010000   i/s:  966.47\n",
      "  step:    6400   loss: 0.973729   lr: 0.010000   i/s:  969.21\n",
      "  step:    6500   loss: 1.370933   lr: 0.010000   i/s: 1012.88\n",
      "  step:    6600   loss: 1.295136   lr: 0.010000   i/s:  980.15\n",
      "  step:    6700   loss: 2.294033   lr: 0.010000   i/s: 1033.45\n",
      "  step:    6800   loss: 0.824266   lr: 0.010000   i/s: 1042.88\n",
      "  step:    6900   loss: 1.632266   lr: 0.010000   i/s: 1035.05\n",
      "  step:    7000   loss: 1.054014   lr: 0.010000   i/s:  974.15\n",
      "Evaluating...\n",
      "  accuracy: 0.8242\n",
      "Training...\n",
      "  step:    7100   loss: 1.058752   lr: 0.010000   i/s:  456.05\n",
      "  step:    7200   loss: 0.871401   lr: 0.010000   i/s:  989.67\n",
      "  step:    7300   loss: 1.888838   lr: 0.010000   i/s: 1043.98\n",
      "Training Epoch 1...\n",
      "  step:    7400   loss: 0.443430   lr: 0.010000   i/s:  913.24\n",
      "  step:    7500   loss: 1.590154   lr: 0.010000   i/s: 1021.41\n",
      "  step:    7600   loss: 1.425861   lr: 0.010000   i/s:  975.07\n",
      "  step:    7700   loss: 0.621780   lr: 0.010000   i/s: 1042.50\n",
      "  step:    7800   loss: 1.464213   lr: 0.010000   i/s: 1009.43\n",
      "  step:    7900   loss: 0.722460   lr: 0.010000   i/s:  985.64\n",
      "  step:    8000   loss: 0.897502   lr: 0.010000   i/s:  987.09\n",
      "Evaluating...\n",
      "  accuracy: 0.8528\n",
      "Training...\n",
      "  step:    8100   loss: 0.753870   lr: 0.010000   i/s:  418.73\n",
      "  step:    8200   loss: 0.759861   lr: 0.010000   i/s: 1035.73\n",
      "  step:    8300   loss: 0.469332   lr: 0.010000   i/s:  990.57\n",
      "  step:    8400   loss: 0.987330   lr: 0.010000   i/s: 1016.41\n",
      "  step:    8500   loss: 0.934088   lr: 0.010000   i/s:  973.05\n",
      "  step:    8600   loss: 0.747963   lr: 0.010000   i/s:  967.62\n",
      "  step:    8700   loss: 0.971788   lr: 0.010000   i/s:  982.59\n",
      "  step:    8800   loss: 0.389881   lr: 0.010000   i/s:  983.42\n",
      "  step:    8900   loss: 0.948968   lr: 0.010000   i/s: 1028.58\n",
      "  step:    9000   loss: 0.761076   lr: 0.010000   i/s: 1017.40\n",
      "Evaluating...\n",
      "  accuracy: 0.8613\n",
      "Training...\n",
      "  step:    9100   loss: 0.925314   lr: 0.010000   i/s:  431.43\n",
      "  step:    9200   loss: 0.409237   lr: 0.010000   i/s: 1032.03\n",
      "  step:    9300   loss: 0.967408   lr: 0.010000   i/s: 1030.05\n",
      "  step:    9400   loss: 0.525338   lr: 0.010000   i/s: 1030.07\n",
      "  step:    9500   loss: 0.758896   lr: 0.010000   i/s: 1038.89\n",
      "  step:    9600   loss: 1.168363   lr: 0.010000   i/s: 1012.51\n",
      "  step:    9700   loss: 0.460460   lr: 0.010000   i/s:  970.48\n",
      "  step:    9800   loss: 0.526057   lr: 0.010000   i/s: 1041.16\n",
      "  step:    9900   loss: 1.087378   lr: 0.010000   i/s: 1040.62\n",
      "  step:   10000   loss: 0.450504   lr: 0.010000   i/s: 1001.08\n",
      "Evaluating...\n",
      "  accuracy: 0.8685\n",
      "Training...\n",
      "  step:   10100   loss: 0.334797   lr: 0.009000   i/s:  403.07\n",
      "  step:   10200   loss: 0.557356   lr: 0.009000   i/s:  973.46\n",
      "  step:   10300   loss: 0.455093   lr: 0.009000   i/s: 1037.15\n",
      "  step:   10400   loss: 0.149972   lr: 0.009000   i/s: 1040.11\n",
      "  step:   10500   loss: 0.450200   lr: 0.009000   i/s: 1041.61\n",
      "  step:   10600   loss: 1.307869   lr: 0.009000   i/s:  981.89\n",
      "  step:   10700   loss: 0.874643   lr: 0.009000   i/s:  958.40\n",
      "  step:   10800   loss: 0.187484   lr: 0.009000   i/s: 1028.29\n",
      "  step:   10900   loss: 0.906819   lr: 0.009000   i/s:  967.84\n",
      "  step:   11000   loss: 0.600397   lr: 0.009000   i/s: 1032.26\n",
      "Evaluating...\n",
      "  accuracy: 0.8581\n",
      "Training...\n",
      "  step:   11100   loss: 0.457243   lr: 0.009000   i/s:  425.77\n",
      "  step:   11200   loss: 0.620792   lr: 0.009000   i/s: 1037.48\n",
      "  step:   11300   loss: 0.788553   lr: 0.009000   i/s: 1004.13\n",
      "  step:   11400   loss: 0.612838   lr: 0.009000   i/s: 1015.23\n",
      "  step:   11500   loss: 0.761436   lr: 0.009000   i/s:  980.06\n",
      "  step:   11600   loss: 1.145185   lr: 0.009000   i/s:  989.70\n",
      "  step:   11700   loss: 0.587807   lr: 0.009000   i/s: 1026.33\n",
      "  step:   11800   loss: 0.374947   lr: 0.009000   i/s:  997.14\n",
      "  step:   11900   loss: 0.535011   lr: 0.009000   i/s:  988.77\n",
      "  step:   12000   loss: 0.919092   lr: 0.009000   i/s: 1035.59\n",
      "Evaluating...\n",
      "  accuracy: 0.8804\n",
      "Training...\n",
      "  step:   12100   loss: 0.530406   lr: 0.009000   i/s:  415.01\n",
      "  step:   12200   loss: 0.943522   lr: 0.009000   i/s: 1014.67\n",
      "  step:   12300   loss: 0.290869   lr: 0.009000   i/s:  983.37\n",
      "  step:   12400   loss: 0.418768   lr: 0.009000   i/s:  973.32\n",
      "  step:   12500   loss: 0.387725   lr: 0.009000   i/s:  973.51\n",
      "  step:   12600   loss: 1.092985   lr: 0.009000   i/s:  930.71\n",
      "  step:   12700   loss: 0.307091   lr: 0.009000   i/s: 1012.94\n",
      "  step:   12800   loss: 0.295358   lr: 0.009000   i/s: 1026.72\n",
      "  step:   12900   loss: 0.608418   lr: 0.009000   i/s: 1016.48\n",
      "  step:   13000   loss: 0.427352   lr: 0.009000   i/s: 1036.79\n",
      "Evaluating...\n",
      "  accuracy: 0.8913\n",
      "Training...\n",
      "  step:   13100   loss: 0.447693   lr: 0.009000   i/s:  400.71\n",
      "  step:   13200   loss: 0.189442   lr: 0.009000   i/s:  995.68\n",
      "  step:   13300   loss: 0.515071   lr: 0.009000   i/s: 1002.99\n",
      "  step:   13400   loss: 0.425247   lr: 0.009000   i/s:  964.00\n",
      "  step:   13500   loss: 0.945437   lr: 0.009000   i/s: 1032.98\n",
      "  step:   13600   loss: 1.293150   lr: 0.009000   i/s: 1033.67\n",
      "  step:   13700   loss: 1.305258   lr: 0.009000   i/s: 1027.65\n",
      "  step:   13800   loss: 0.362586   lr: 0.009000   i/s: 1016.37\n",
      "  step:   13900   loss: 0.403313   lr: 0.009000   i/s: 1024.22\n",
      "  step:   14000   loss: 0.399584   lr: 0.009000   i/s: 1005.55\n",
      "Evaluating...\n",
      "  accuracy: 0.8864\n",
      "Training...\n",
      "  step:   14100   loss: 0.207920   lr: 0.009000   i/s:  421.31\n",
      "  step:   14200   loss: 0.574743   lr: 0.009000   i/s: 1032.52\n",
      "  step:   14300   loss: 1.355771   lr: 0.009000   i/s: 1029.60\n",
      "  step:   14400   loss: 0.892763   lr: 0.009000   i/s: 1027.03\n",
      "  step:   14500   loss: 0.755069   lr: 0.009000   i/s: 1024.56\n",
      "  step:   14600   loss: 0.611209   lr: 0.009000   i/s: 1027.53\n",
      "  step:   14700   loss: 1.013487   lr: 0.009000   i/s: 1030.17\n",
      "Training Epoch 2...\n",
      "  step:   14800   loss: 0.242172   lr: 0.009000   i/s:  915.02\n",
      "  step:   14900   loss: 0.575221   lr: 0.009000   i/s: 1039.00\n",
      "  step:   15000   loss: 1.704033   lr: 0.009000   i/s: 1042.70\n",
      "Evaluating...\n",
      "  accuracy: 0.8921\n",
      "Training...\n",
      "  step:   15100   loss: 0.189718   lr: 0.009000   i/s:  438.08\n",
      "  step:   15200   loss: 0.234260   lr: 0.009000   i/s: 1039.08\n",
      "  step:   15300   loss: 0.443983   lr: 0.009000   i/s: 1035.92\n",
      "  step:   15400   loss: 0.443126   lr: 0.009000   i/s: 1035.63\n",
      "  step:   15500   loss: 0.282271   lr: 0.009000   i/s:  990.64\n",
      "  step:   15600   loss: 1.088677   lr: 0.009000   i/s: 1038.22\n",
      "  step:   15700   loss: 0.519746   lr: 0.009000   i/s: 1035.23\n",
      "  step:   15800   loss: 0.681998   lr: 0.009000   i/s: 1037.49\n",
      "  step:   15900   loss: 0.525874   lr: 0.009000   i/s: 1037.64\n",
      "  step:   16000   loss: 0.427543   lr: 0.009000   i/s: 1033.11\n",
      "Evaluating...\n",
      "  accuracy: 0.8983\n",
      "Training...\n",
      "  step:   16100   loss: 0.556764   lr: 0.009000   i/s:  439.16\n",
      "  step:   16200   loss: 0.208572   lr: 0.009000   i/s: 1035.11\n",
      "  step:   16300   loss: 1.919777   lr: 0.009000   i/s: 1042.30\n",
      "  step:   16400   loss: 0.510058   lr: 0.009000   i/s: 1042.38\n",
      "  step:   16500   loss: 0.419661   lr: 0.009000   i/s: 1034.21\n",
      "  step:   16600   loss: 0.799362   lr: 0.009000   i/s: 1039.67\n",
      "  step:   16700   loss: 0.204670   lr: 0.009000   i/s:  997.10\n",
      "  step:   16800   loss: 0.500988   lr: 0.009000   i/s: 1030.67\n",
      "  step:   16900   loss: 0.712977   lr: 0.009000   i/s: 1033.84\n",
      "  step:   17000   loss: 0.174459   lr: 0.009000   i/s: 1035.19\n",
      "Evaluating...\n",
      "  accuracy: 0.8957\n",
      "Training...\n",
      "  step:   17100   loss: 0.225670   lr: 0.009000   i/s:  450.50\n",
      "  step:   17200   loss: 0.604853   lr: 0.009000   i/s: 1034.77\n",
      "  step:   17300   loss: 0.238381   lr: 0.009000   i/s: 1037.01\n",
      "  step:   17400   loss: 0.287099   lr: 0.009000   i/s: 1038.43\n",
      "  step:   17500   loss: 1.350337   lr: 0.009000   i/s: 1041.36\n",
      "  step:   17600   loss: 0.323076   lr: 0.009000   i/s: 1039.37\n",
      "  step:   17700   loss: 1.056839   lr: 0.009000   i/s: 1039.22\n",
      "  step:   17800   loss: 0.290084   lr: 0.009000   i/s:  985.49\n",
      "  step:   17900   loss: 0.265543   lr: 0.009000   i/s: 1027.72\n",
      "  step:   18000   loss: 0.369357   lr: 0.009000   i/s: 1034.57\n",
      "Evaluating...\n",
      "  accuracy: 0.8848\n",
      "Training...\n",
      "  step:   18100   loss: 0.293914   lr: 0.009000   i/s:  438.93\n",
      "  step:   18200   loss: 0.681480   lr: 0.009000   i/s: 1031.91\n",
      "  step:   18300   loss: 0.275915   lr: 0.009000   i/s: 1035.97\n",
      "  step:   18400   loss: 0.516870   lr: 0.009000   i/s: 1040.32\n",
      "  step:   18500   loss: 0.512865   lr: 0.009000   i/s: 1038.09\n",
      "  step:   18600   loss: 0.279998   lr: 0.009000   i/s: 1038.29\n",
      "  step:   18700   loss: 0.188850   lr: 0.009000   i/s: 1035.71\n",
      "  step:   18800   loss: 0.581584   lr: 0.009000   i/s: 1037.45\n",
      "  step:   18900   loss: 0.754903   lr: 0.009000   i/s: 1039.61\n",
      "  step:   19000   loss: 0.587499   lr: 0.009000   i/s: 1039.13\n",
      "Evaluating...\n",
      "  accuracy: 0.8904\n",
      "Training...\n",
      "  step:   19100   loss: 0.214593   lr: 0.009000   i/s:  444.66\n",
      "  step:   19200   loss: 0.824621   lr: 0.009000   i/s: 1039.32\n",
      "  step:   19300   loss: 0.639391   lr: 0.009000   i/s: 1036.49\n",
      "  step:   19400   loss: 0.307804   lr: 0.009000   i/s: 1038.45\n",
      "  step:   19500   loss: 0.170342   lr: 0.009000   i/s: 1039.58\n",
      "  step:   19600   loss: 0.442691   lr: 0.009000   i/s: 1039.51\n",
      "  step:   19700   loss: 0.656529   lr: 0.009000   i/s: 1031.29\n",
      "  step:   19800   loss: 0.497308   lr: 0.009000   i/s:  996.97\n",
      "  step:   19900   loss: 0.390402   lr: 0.009000   i/s: 1030.80\n",
      "  step:   20000   loss: 0.181043   lr: 0.009000   i/s: 1025.86\n",
      "Evaluating...\n",
      "  accuracy: 0.9037\n",
      "Training...\n",
      "  step:   20100   loss: 0.279833   lr: 0.008100   i/s:  420.01\n",
      "  step:   20200   loss: 0.387441   lr: 0.008100   i/s: 1026.11\n",
      "  step:   20300   loss: 0.614445   lr: 0.008100   i/s: 1035.44\n",
      "  step:   20400   loss: 1.887640   lr: 0.008100   i/s: 1035.64\n",
      "  step:   20500   loss: 0.467831   lr: 0.008100   i/s: 1035.79\n",
      "  step:   20600   loss: 0.890244   lr: 0.008100   i/s: 1022.09\n",
      "  step:   20700   loss: 0.299031   lr: 0.008100   i/s: 1020.53\n",
      "  step:   20800   loss: 0.341366   lr: 0.008100   i/s: 1035.15\n",
      "  step:   20900   loss: 0.367442   lr: 0.008100   i/s: 1026.69\n",
      "  step:   21000   loss: 0.216183   lr: 0.008100   i/s: 1020.25\n",
      "Evaluating...\n",
      "  accuracy: 0.8963\n",
      "Training...\n",
      "  step:   21100   loss: 1.481229   lr: 0.008100   i/s:  454.19\n",
      "  step:   21200   loss: 0.068218   lr: 0.008100   i/s: 1037.78\n",
      "  step:   21300   loss: 0.410192   lr: 0.008100   i/s: 1038.15\n",
      "  step:   21400   loss: 0.807058   lr: 0.008100   i/s: 1037.22\n",
      "  step:   21500   loss: 0.477383   lr: 0.008100   i/s: 1036.73\n",
      "  step:   21600   loss: 1.209804   lr: 0.008100   i/s: 1037.13\n",
      "  step:   21700   loss: 1.116107   lr: 0.008100   i/s: 1040.38\n",
      "  step:   21800   loss: 0.131164   lr: 0.008100   i/s: 1042.21\n",
      "  step:   21900   loss: 1.593124   lr: 0.008100   i/s: 1035.39\n",
      "  step:   22000   loss: 0.226311   lr: 0.008100   i/s: 1015.62\n",
      "Evaluating...\n",
      "  accuracy: 0.9071\n",
      "Training...\n",
      "  step:   22100   loss: 0.339608   lr: 0.008100   i/s:  422.11\n"
     ]
    }
   ],
   "source": [
    "print_every = 100\n",
    "eval_every = 1000\n",
    "\n",
    "step = 0\n",
    "best_accuracy = 0\n",
    "time_start = time.time()\n",
    "\n",
    "for epoch in range(3):\n",
    "    \n",
    "    print(f'Training Epoch {epoch}...')\n",
    "    model.train()\n",
    "    for images, targets in train_loader:\n",
    "        images = images.to(device)\n",
    "        targets = [t.to(device) for t in targets]\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = custom_loss(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "        step += 1\n",
    "\n",
    "        if step % print_every == 0:\n",
    "            dtnow = datetime.datetime.now()\n",
    "            lr = scheduler.get_lr()[0]\n",
    "            dtime = time.time() - time_start\n",
    "            img_per_sec = batch_size * print_every / dtime\n",
    "            print(f'  step: {step:7d}   loss: {loss.item():.6f}   lr: {lr:.6f}   i/s: {img_per_sec:7.2f}')\n",
    "            time_start = time.time()\n",
    "        \n",
    "        if step % eval_every == 0:\n",
    "            \n",
    "            print('Evaluating...')\n",
    "            accuracy = evaluate(model, valid_loader)\n",
    "            print(f'  accuracy: {accuracy:.4f}')\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                model_path = pathlib.PosixPath(model_save_location)/f'model_{step}'\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            print('Training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVHNModel()\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load(model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:svhn]",
   "language": "python",
   "name": "conda-env-svhn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
