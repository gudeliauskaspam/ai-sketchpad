{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents **Many-to-Many** achitecture based on **Bidirectional LSTM** cells. Neural network is used to learn **English to French** translation task on a small corpus of sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/rnn_bidirectional.png\"/>\n",
    "<center>Figure from Bidirectional Recurrent Neural Networks (1997) by Mike Schuster and kuldip K. Paliwal</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "\n",
    "* [Udacity NLP Nanodegree](https://eu.udacity.com/course/natural-language-processing-nanodegree--nd892) - I found dataset as part of the course\n",
    "* [Udacity NLP GitHub](https://github.com/udacity/aind2-nlp-capstone) - dataset link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "\n",
    "* [Bidirectional Recurrent Neural Networks](https://ieeexplore.ieee.org/document/650093) (1997) by Mike Schuster and kuldip K. Paliwal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit TensorFlow GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)  # init TF ...\n",
    "config=tf.ConfigProto(gpu_options=gpu_options)  # w/o taking ...\n",
    "with tf.Session(config=config): pass            # all GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English to French Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from the link in the introduction and point path below to folder with *small_vocab_en* and *small_vocab_fr*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = '/home/marcin/Udacity/NLPND/aind2-nlp-capstone/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*small_vocab_en* contains approx 137860 short sentences in English. *small_vocab_fr* contains corresponding sentences in french."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 137860\n",
      "example sentences:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ST new jersey is sometimes quiet during autumn , and it is snowy in april . EN',\n",
       " 'ST the united states is usually chilly during july , and it is usually freezing in november . EN',\n",
       " 'ST california is usually quiet during march , and it is usually hot in june . EN']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(dataset_location, 'small_vocab_en')) as f:\n",
    "    # line below: 1) reads lines from file, 2) strips /n char and converts to lowercase, 3) adds special start/end words\n",
    "    data_en_raw = list(map(lambda x: 'ST '+x.strip().lower()+' EN', f.readlines()))\n",
    "print('len:', len(data_en_raw))\n",
    "print('example sentences:')\n",
    "data_en_raw[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 137860\n",
      "example sentences:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"ST new jersey est parfois calme pendant l' automne , et il est neigeux en avril . EN\",\n",
       " 'ST les états-unis est généralement froid en juillet , et il gèle habituellement en novembre . EN',\n",
       " 'ST california est généralement calme en mars , et il est généralement chaud en juin . EN']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(dataset_location, 'small_vocab_fr')) as f:\n",
    "    # line below: 1) reads lines from file, 2) strips /n char and converts to lowercase, 3) adds special start/end words\n",
    "    data_fr_raw = list(map(lambda x: 'ST '+x.strip().lower()+' EN', f.readlines()))\n",
    "print('len:', len(data_fr_raw))\n",
    "print('example sentences:')\n",
    "data_fr_raw[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Keras tokenizer to convert text sentences to tokens. Each word gets it's own unique integer token. Special words ST/EN also get their tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_en = tf.keras.preprocessing.text.Tokenizer(lower=False)\n",
    "tok_en.fit_on_texts(data_en_raw)\n",
    "data_en_tok = tok_en.texts_to_sequences(data_en_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example tokens for English:\n",
      "is: 1     ST: 2     EN: 3     in: 4     it: 5\n",
      "example sentences after tokenization:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 19, 25, 1, 10, 69, 6, 41, 9, 5, 1, 57, 4, 46, 3],\n",
       " [2, 7, 22, 23, 1, 11, 64, 6, 45, 9, 5, 1, 11, 53, 4, 47, 3],\n",
       " [2, 24, 1, 11, 69, 6, 40, 9, 5, 1, 11, 70, 4, 36, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('example tokens for English:')\n",
    "print('is:', tok_en.word_index['is'], '   ',\n",
    "      'ST:', tok_en.word_index['ST'], '   ',\n",
    "      'EN:', tok_en.word_index['EN'], '   ',\n",
    "      'in:', tok_en.word_index['in'], '   ',\n",
    "      'it:', tok_en.word_index['it'])\n",
    "print('example sentences after tokenization:')\n",
    "data_en_tok[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_fr = tf.keras.preprocessing.text.Tokenizer(lower=False)\n",
    "tok_fr.fit_on_texts(data_fr_raw)\n",
    "data_fr_tok = tok_fr.texts_to_sequences(data_fr_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example tokens for French:\n",
      "est: 1     ST: 2     EN: 3     en: 4     il: 5\n",
      "example sentences after tokenization:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 37, 36, 1, 10, 69, 39, 13, 26, 8, 5, 1, 114, 4, 52, 3],\n",
       " [2, 6, 34, 33, 1, 14, 21, 4, 51, 8, 5, 97, 71, 4, 53, 3],\n",
       " [2, 103, 1, 14, 69, 4, 47, 8, 5, 1, 14, 23, 4, 43, 3]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('example tokens for French:')\n",
    "print('est:', tok_fr.word_index['est'], '   ',\n",
    "      'ST:', tok_fr.word_index['ST'], '   ',\n",
    "      'EN:', tok_fr.word_index['EN'], '   ',\n",
    "      'en:', tok_fr.word_index['en'], '   ',\n",
    "      'il:', tok_fr.word_index['il'])\n",
    "print('example sentences after tokenization:')\n",
    "data_fr_tok[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate maximum sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length in either English or French: 23 tokens (including EN/ST)\n"
     ]
    }
   ],
   "source": [
    "max_len_en = len(max(data_en_tok, key=len))\n",
    "max_len_fr = len(max(data_fr_tok, key=len))\n",
    "max_len_both = max(max_len_en, max_len_fr)\n",
    "print('Maximum sentence length in either English or French:', max_len_both, 'tokens (including EN/ST)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad both corpuses to longest sentence - input and output lenghts need to match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en_tok, maxlen=max_len_both, padding='post')\n",
    "data_fr = tf.keras.preprocessing.sequence.pad_sequences(data_fr_tok, maxlen=max_len_both, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length English sentence (tokens):    23\n",
      "Max length French sentence (tokens):     23\n",
      "Num tokens in English vocabulary:        201\n",
      "Num tokens in English vocabulary:        346\n"
     ]
    }
   ],
   "source": [
    "n_en_seq = data_en.shape[1]\n",
    "n_fr_seq = data_fr.shape[1]\n",
    "n_en_vocab = len(tok_en.word_index)\n",
    "n_fr_vocab = len(tok_fr.word_index)\n",
    "max_seq_len = max(n_en_seq, n_fr_seq)\n",
    "print('Max length English sentence (tokens):   ', n_en_seq)\n",
    "print('Max length French sentence (tokens):    ', n_fr_seq)\n",
    "print('Num tokens in English vocabulary:       ', n_en_vocab)\n",
    "print('Num tokens in English vocabulary:       ', n_fr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English train data\n",
      "shape: (137860, 23)\n",
      "[[ 2 31 14 18 15  1  7 84  8 32 14 18  1  7 85  3  0  0  0  0  0  0  0]\n",
      " [ 2 33 13 15  1  7 86  8 32 13  1  7 84  3  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 20  1 68  6 49  8  5  1 11 64  4 45  3  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print('English train data')\n",
    "print('shape:', data_en.shape)\n",
    "print(data_en[4:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French train targets data\n",
      "shape: (137860, 23)\n",
      "[[ 2 42 15 16 18  1 12 84  7 41 15 16  1  9 85  3  0  0  0  0  0  0  0]\n",
      " [ 2 22 18 19  1 86  7 41 19  1 12 84  3  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2 31  1 60  4 54  7  5  1 14 21  4 51  3  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print('French train targets data')\n",
    "print('shape:', data_fr.shape)\n",
    "print(data_fr[4:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, GRU, TimeDistributed, Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 23)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 23, 50)            10050     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 23, 128)           44160     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 23, 346)           44634     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 23, 346)           0         \n",
      "=================================================================\n",
      "Total params: 98,844\n",
      "Trainable params: 98,844\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, GRU, TimeDistributed, Dense, Activation\n",
    "\n",
    "X_input = Input(shape=(n_en_seq,))\n",
    "X = Embedding(input_dim=n_en_vocab, output_dim=50)(X_input)\n",
    "X = Bidirectional( GRU(units=64, return_sequences=True) )(X)\n",
    "X = TimeDistributed(Dense(units=n_fr_vocab))(X)\n",
    "X = Activation('softmax')(X)\n",
    "\n",
    "model = tf.keras.Model(inputs=X_input, outputs=X)\n",
    "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcin/.anaconda/envs/keras/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27572 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 18s 166us/step - loss: 3.5178 - sparse_categorical_accuracy: 0.4004 - val_loss: nan - val_sparse_categorical_accuracy: 0.4766\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 9s 79us/step - loss: 2.3494 - sparse_categorical_accuracy: 0.4932 - val_loss: nan - val_sparse_categorical_accuracy: 0.5274\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 9s 77us/step - loss: 1.6421 - sparse_categorical_accuracy: 0.6007 - val_loss: nan - val_sparse_categorical_accuracy: 0.6514\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 9s 77us/step - loss: 1.2320 - sparse_categorical_accuracy: 0.6817 - val_loss: nan - val_sparse_categorical_accuracy: 0.7173\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 8s 77us/step - loss: 0.9896 - sparse_categorical_accuracy: 0.7387 - val_loss: nan - val_sparse_categorical_accuracy: 0.7548\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 9s 78us/step - loss: 0.8284 - sparse_categorical_accuracy: 0.7678 - val_loss: nan - val_sparse_categorical_accuracy: 0.7821\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 9s 78us/step - loss: 0.7130 - sparse_categorical_accuracy: 0.7951 - val_loss: nan - val_sparse_categorical_accuracy: 0.8072\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 9s 78us/step - loss: 0.6264 - sparse_categorical_accuracy: 0.8165 - val_loss: nan - val_sparse_categorical_accuracy: 0.8264\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 9s 77us/step - loss: 0.5584 - sparse_categorical_accuracy: 0.8344 - val_loss: nan - val_sparse_categorical_accuracy: 0.8427\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 9s 78us/step - loss: 0.5043 - sparse_categorical_accuracy: 0.8492 - val_loss: nan - val_sparse_categorical_accuracy: 0.8573\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=data_en, y=np.expand_dims(data_fr, axis=-1),\n",
    "                 batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to convert tokenized sentence back to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_french(seq):\n",
    "    words = [tok_fr.index_word[x] for x in seq if x in tok_fr.index_word]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english:             ST we dislike oranges , grapefruit , and bananas . EN\n",
      "french (original):   ST nous détestons les oranges , le pamplemousse et les bananes . EN\n",
      "french (predicted):  ST nous détestons les le pamplemousse pamplemousse et les bananes EN\n"
     ]
    }
   ],
   "source": [
    "index = 234\n",
    "english_sentence = data_en_raw[index]\n",
    "french_sentence = data_fr_raw[index]\n",
    "\n",
    "prediction_prob = model.predict(data_en[index:index+1])\n",
    "prediction_prob = prediction_prob.squeeze()\n",
    "prediction_tok = prediction_prob.argmax(axis=-1)\n",
    "predicted_sentence = sequence_to_french(prediction_tok)\n",
    "\n",
    "print('english:            ', english_sentence)\n",
    "print('french (original):  ', french_sentence)\n",
    "print('french (predicted): ', predicted_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
