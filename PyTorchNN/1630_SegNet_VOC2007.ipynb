{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "  Originally copied from https://github.com/Sayan98/pytorch-segnet\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch\n",
    "import segnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOC2007 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change me\n",
    "\n",
    "VOC_CLASSES = ('background',  # always index 0\n",
    "               'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "\n",
    "NUM_CLASSES = len(VOC_CLASSES) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOC_CLASSES = ('background',  # always index 0\n",
    "               'pipe')\n",
    "\n",
    "NUM_CLASSES = len(VOC_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change me\n",
    "\n",
    "class PascalVOCDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Pascal VOC 2007 Dataset\"\"\"\n",
    "    def __init__(self, list_file, img_dir, mask_dir, transform=None):\n",
    "        self.images = open(list_file, \"rt\").read().split(\"\\n\")[:-1]\n",
    "        self.transform = transform\n",
    "\n",
    "        self.img_extension = \".jpg\"\n",
    "        self.mask_extension = \".png\"\n",
    "\n",
    "        self.image_root_dir = img_dir\n",
    "        self.mask_root_dir = mask_dir\n",
    "\n",
    "        self.counts = self.__compute_class_probability()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.images[index]\n",
    "        image_path = os.path.join(self.image_root_dir, name + self.img_extension)\n",
    "        mask_path = os.path.join(self.mask_root_dir, name + self.mask_extension)\n",
    "\n",
    "        image = self.load_image(path=image_path)\n",
    "        gt_mask = self.load_mask(path=mask_path)\n",
    "\n",
    "        data = {\n",
    "                    'image': torch.FloatTensor(image),\n",
    "                    'mask' : torch.LongTensor(gt_mask)\n",
    "                    }\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __compute_class_probability(self):\n",
    "        counts = dict((i, 0) for i in range(NUM_CLASSES))\n",
    "\n",
    "        for name in self.images:\n",
    "            mask_path = os.path.join(self.mask_root_dir, name + self.mask_extension)\n",
    "\n",
    "            raw_image = PIL.Image.open(mask_path).resize((224, 224))\n",
    "            imx_t = np.array(raw_image).reshape(224*224)\n",
    "            imx_t[imx_t==255] = len(VOC_CLASSES)\n",
    "\n",
    "            for i in range(NUM_CLASSES):\n",
    "                counts[i] += np.sum(imx_t == i)\n",
    "\n",
    "        return counts\n",
    "\n",
    "    def get_class_probability(self):\n",
    "        values = np.array(list(self.counts.values()))\n",
    "        p_values = values/np.sum(values)\n",
    "\n",
    "        return torch.Tensor(p_values)\n",
    "\n",
    "    def load_image(self, path=None):\n",
    "        raw_image = PIL.Image.open(path)\n",
    "        raw_image = np.transpose(raw_image.resize((224, 224)), (2,1,0))\n",
    "        imx_t = np.array(raw_image, dtype=np.float32)/255.0\n",
    "\n",
    "        return imx_t\n",
    "\n",
    "    def load_mask(self, path=None):\n",
    "        raw_image = PIL.Image.open(path)\n",
    "        raw_image = raw_image.resize((224, 224))\n",
    "        imx_t = np.array(raw_image)\n",
    "        # border\n",
    "        imx_t[imx_t==255] = len(VOC_CLASSES)\n",
    "\n",
    "        return imx_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/marcin/Datasets/VOC2007'\n",
    "train_txt, val_txt = 'train_mini.txt', 'val_mini.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/home/marcin/Datasets/rovco/dataset'\n",
    "train_txt, val_txt = 'train.txt', 'val.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(data_root, 'ImageSets/Segmentation', train_txt)\n",
    "val_path = os.path.join(data_root, 'ImageSets/Segmentation/', val_txt)\n",
    "img_dir = os.path.join(data_root, \"JPEGImages\")\n",
    "mask_dir = os.path.join(data_root, \"SegmentationClass\")\n",
    "\n",
    "save_dir = './savedir'\n",
    "checkpoint = None\n",
    "\n",
    "CUDA = True  # args.gpu is not None\n",
    "GPU_ID = 0   # args.gpu\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PascalVOCDataset(list_file=train_path,\n",
    "                                 img_dir=img_dir,\n",
    "                                 mask_dir=mask_dir)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.get_class_probability())\n",
    "\n",
    "sample = train_dataset[11]\n",
    "image, mask = sample['image'], sample['mask']\n",
    "\n",
    "image.transpose_(0, 2)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "a = fig.add_subplot(1,2,1)\n",
    "plt.imshow(image)\n",
    "\n",
    "a = fig.add_subplot(1,2,2)\n",
    "plt.imshow(mask)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_on_random_input = False\n",
    "\n",
    "if test_model_on_random_input:\n",
    "\n",
    "    # Model\n",
    "    model = segnet.SegNet(input_channels=3, output_channels=NUM_CLASSES)\n",
    "\n",
    "    # print(model)\n",
    "\n",
    "    img = torch.randn([4, 3, 224, 224])\n",
    "    output, softmaxed_output = model(img)\n",
    "\n",
    "    print(output.size())\n",
    "    print(softmaxed_output.size())\n",
    "\n",
    "    print(output[0,:,0,0])\n",
    "    print(softmaxed_output[0,:,0,0].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change me\n",
    "\n",
    "# Constants\n",
    "NUM_INPUT_CHANNELS = 3\n",
    "NUM_OUTPUT_CHANNELS = NUM_CLASSES\n",
    "\n",
    "NUM_EPOCHS = 6000\n",
    "\n",
    "LEARNING_RATE = 1e-3 # 1e-6\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    model = segnet.SegNet(input_channels=NUM_INPUT_CHANNELS,\n",
    "                          output_channels=NUM_OUTPUT_CHANNELS).cuda(GPU_ID)\n",
    "\n",
    "    class_weights = 1.0/train_dataset.get_class_probability().cuda(GPU_ID)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights).cuda(GPU_ID)\n",
    "else:\n",
    "    model = segnet.SegNet(input_channels=NUM_INPUT_CHANNELS,\n",
    "                          output_channels=NUM_OUTPUT_CHANNELS)\n",
    "\n",
    "    class_weights = 1.0/train_dataset.get_class_probability()\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "    \n",
    "if checkpoint:\n",
    "    model.load_state_dict(torch.load(args.checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    is_better = True\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss_f = 0\n",
    "        t_start = time.time()\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            input_tensor = torch.autograd.Variable(batch['image'])\n",
    "            target_tensor = torch.autograd.Variable(batch['mask'])\n",
    "\n",
    "            if CUDA:\n",
    "                input_tensor = input_tensor.cuda(GPU_ID)\n",
    "                target_tensor = target_tensor.cuda(GPU_ID)\n",
    "\n",
    "            predicted_tensor, softmaxed_tensor = model(input_tensor)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(softmaxed_tensor, target_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            loss_f += loss.float()\n",
    "            prediction_f = softmaxed_tensor.float()\n",
    "\n",
    "        delta = time.time() - t_start\n",
    "        is_better = loss_f < prev_loss\n",
    "\n",
    "        if is_better:\n",
    "            prev_loss = loss_f\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, \"model_best.pth\"))\n",
    "\n",
    "        print(\"Epoch #{}\\tLoss: {:.8f}\\t Time: {:2f}s\".format(epoch+1, loss_f, delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\tLoss: 0.35000780\t Time: 0.525096s\n",
      "Epoch #2\tLoss: 0.34862185\t Time: 0.524583s\n",
      "Epoch #3\tLoss: 0.34821707\t Time: 0.516271s\n",
      "Epoch #4\tLoss: 0.34586528\t Time: 0.516663s\n",
      "Epoch #5\tLoss: 0.34524012\t Time: 0.519219s\n",
      "Epoch #6\tLoss: 0.34394974\t Time: 0.520299s\n",
      "Epoch #7\tLoss: 0.34403336\t Time: 0.524212s\n",
      "Epoch #8\tLoss: 0.34283078\t Time: 0.511041s\n",
      "Epoch #9\tLoss: 0.33954406\t Time: 0.516970s\n",
      "Epoch #10\tLoss: 0.34080076\t Time: 0.515382s\n",
      "Epoch #11\tLoss: 0.33785093\t Time: 0.505640s\n",
      "Epoch #12\tLoss: 0.34325865\t Time: 0.661593s\n",
      "Epoch #13\tLoss: 0.36328009\t Time: 0.507857s\n",
      "Epoch #14\tLoss: 0.34938964\t Time: 0.514261s\n",
      "Epoch #15\tLoss: 0.34107235\t Time: 0.519991s\n",
      "Epoch #16\tLoss: 0.34347156\t Time: 0.519263s\n",
      "Epoch #17\tLoss: 0.34604922\t Time: 0.514745s\n",
      "Epoch #18\tLoss: 0.34335804\t Time: 0.514310s\n",
      "Epoch #19\tLoss: 0.33804953\t Time: 0.526909s\n",
      "Epoch #20\tLoss: 0.33506152\t Time: 0.511319s\n",
      "Epoch #21\tLoss: 0.33755091\t Time: 0.514496s\n",
      "Epoch #22\tLoss: 0.33654803\t Time: 0.520099s\n",
      "Epoch #23\tLoss: 0.33374000\t Time: 0.513131s\n",
      "Epoch #24\tLoss: 0.33372012\t Time: 0.514681s\n",
      "Epoch #25\tLoss: 0.33272848\t Time: 0.514851s\n",
      "Epoch #26\tLoss: 0.33115527\t Time: 0.517109s\n",
      "Epoch #27\tLoss: 0.32961243\t Time: 0.511105s\n",
      "Epoch #28\tLoss: 0.32845253\t Time: 0.515631s\n",
      "Epoch #29\tLoss: 0.32888499\t Time: 0.524811s\n",
      "Epoch #30\tLoss: 0.32859713\t Time: 0.519887s\n",
      "Epoch #31\tLoss: 0.32896572\t Time: 0.520395s\n",
      "Epoch #32\tLoss: 0.34223297\t Time: 0.513973s\n",
      "Epoch #33\tLoss: 0.33854637\t Time: 0.515557s\n",
      "Epoch #34\tLoss: 0.33039632\t Time: 0.514873s\n",
      "Epoch #35\tLoss: 0.33297119\t Time: 0.517611s\n",
      "Epoch #36\tLoss: 0.33403856\t Time: 0.513127s\n",
      "Epoch #37\tLoss: 0.33270967\t Time: 0.526665s\n",
      "Epoch #38\tLoss: 0.33040074\t Time: 0.524718s\n",
      "Epoch #39\tLoss: 0.33058646\t Time: 0.515824s\n",
      "Epoch #40\tLoss: 0.32845187\t Time: 0.513859s\n",
      "Epoch #41\tLoss: 0.32600623\t Time: 0.536450s\n",
      "Epoch #42\tLoss: 0.32415164\t Time: 0.518914s\n",
      "Epoch #43\tLoss: 0.32583639\t Time: 0.524059s\n",
      "Epoch #44\tLoss: 0.33436805\t Time: 0.517608s\n",
      "Epoch #45\tLoss: 0.36854926\t Time: 0.512733s\n",
      "Epoch #46\tLoss: 0.32918960\t Time: 0.515152s\n",
      "Epoch #47\tLoss: 0.34235951\t Time: 0.519264s\n",
      "Epoch #48\tLoss: 0.33419308\t Time: 0.519037s\n",
      "Epoch #49\tLoss: 0.34231856\t Time: 0.518532s\n",
      "Epoch #50\tLoss: 0.33883491\t Time: 0.520469s\n",
      "Epoch #51\tLoss: 0.33933350\t Time: 0.520929s\n",
      "Epoch #52\tLoss: 0.34066305\t Time: 0.516779s\n",
      "Epoch #53\tLoss: 0.33910173\t Time: 0.515452s\n",
      "Epoch #54\tLoss: 0.33571973\t Time: 0.512647s\n",
      "Epoch #55\tLoss: 0.33390537\t Time: 0.518788s\n",
      "Epoch #56\tLoss: 0.33212170\t Time: 0.516750s\n",
      "Epoch #57\tLoss: 0.33166543\t Time: 0.518491s\n",
      "Epoch #58\tLoss: 0.32949400\t Time: 0.516167s\n",
      "Epoch #59\tLoss: 0.32874453\t Time: 0.539754s\n",
      "Epoch #60\tLoss: 0.32658252\t Time: 0.516739s\n",
      "Epoch #61\tLoss: 0.32600027\t Time: 0.512550s\n",
      "Epoch #62\tLoss: 0.32572681\t Time: 0.517205s\n",
      "Epoch #63\tLoss: 0.32355458\t Time: 0.511459s\n",
      "Epoch #64\tLoss: 0.32433796\t Time: 0.517109s\n",
      "Epoch #65\tLoss: 0.32405674\t Time: 0.512277s\n",
      "Epoch #66\tLoss: 0.32347411\t Time: 0.512807s\n",
      "Epoch #67\tLoss: 0.32265654\t Time: 0.522398s\n",
      "Epoch #68\tLoss: 0.32167575\t Time: 0.516287s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-f82278d2d2db>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mis_better\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_f\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprev_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mis_better\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mprev_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_best.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, batch in enumerate(val_dataloader):\n",
    "        input_tensor = torch.autograd.Variable(batch['image'])\n",
    "        target_tensor = torch.autograd.Variable(batch['mask'])\n",
    "\n",
    "        if CUDA:\n",
    "            input_tensor = input_tensor.cuda(GPU_ID)\n",
    "            target_tensor = target_tensor.cuda(GPU_ID)\n",
    "\n",
    "        predicted_tensor, softmaxed_tensor = model(input_tensor)\n",
    "        loss = criterion(predicted_tensor, target_tensor)\n",
    "\n",
    "        for idx, predicted_mask in enumerate(softmaxed_tensor):\n",
    "            target_mask = target_tensor[idx]\n",
    "            input_image = input_tensor[idx]\n",
    "\n",
    "            fig = plt.figure()\n",
    "\n",
    "            a = fig.add_subplot(1,3,1)\n",
    "            plt.imshow(input_image.transpose(0, 2).cpu().numpy())\n",
    "            a.set_title('Input Image')\n",
    "\n",
    "            a = fig.add_subplot(1,3,2)\n",
    "            predicted_mx = predicted_mask.detach().cpu().numpy()\n",
    "            predicted_mx = predicted_mx.argmax(axis=0)\n",
    "            plt.imshow(predicted_mx)\n",
    "            a.set_title('Predicted Mask')\n",
    "\n",
    "            a = fig.add_subplot(1,3,3)\n",
    "            target_mx = target_mask.detach().cpu().numpy()\n",
    "            plt.imshow(target_mx)\n",
    "            a.set_title('Ground Truth')\n",
    "\n",
    "            #fig.savefig(os.path.join(OUTPUT_DIR, \"prediction_{}_{}.png\".format(batch_idx, idx)))\n",
    "\n",
    "            #plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SAVED_MODEL_PATH = './savedir/model_best.pth'\n",
    "\n",
    "val_dataset = PascalVOCDataset(list_file=val_path,\n",
    "                               img_dir=img_dir,\n",
    "                               mask_dir=mask_dir)\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if CUDA:\n",
    "#     model = segnet.SegNet(input_channels=NUM_INPUT_CHANNELS,\n",
    "#                           output_channels=NUM_OUTPUT_CHANNELS).cuda(GPU_ID)\n",
    "\n",
    "#     class_weights = 1.0/val_dataset.get_class_probability().cuda(GPU_ID)\n",
    "#     criterion = torch.nn.CrossEntropyLoss(weight=class_weights).cuda(GPU_ID)\n",
    "# else:\n",
    "#     model = segnet.SegNet(input_channels=NUM_INPUT_CHANNELS,\n",
    "#                           output_channels=NUM_OUTPUT_CHANNELS)\n",
    "\n",
    "#     class_weights = 1.0/val_dataset.get_class_probability()\n",
    "#     criterion = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(SAVED_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
