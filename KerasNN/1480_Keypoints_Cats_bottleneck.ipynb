{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents **Keypoint Detection** using a **CNN** on the [CAT](https://www.kaggle.com/crawford/cat-dataset) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "\n",
    "* [Cat Hipsterizer](https://github.com/kairess/cat_hipsterizer) - inspiration for this notebook\n",
    "* [CAT (Kaggle)](https://www.kaggle.com/crawford/cat-dataset) - dataset used in this notebook\n",
    "* [CAT (original)](https://archive.org/details/CAT_DATASET) - internet archive link to original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import PIL.ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit TensorFlow GPU memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)  # init TF ...\n",
    "config=tf.ConfigProto(gpu_options=gpu_options)  # w/o taking ...\n",
    "with tf.Session(config=config): pass            # all GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point this to dataset directory, folder should contain CAT_00, CAT_01 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = '/home/marcin/Datasets/cat-dataset/cats/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(indices, images, features):\n",
    "    def draw_keypoints(img, keypoints, r=2, c='red'):\n",
    "        \"\"\"Draw keypoints on PIL image\"\"\"\n",
    "        draw = PIL.ImageDraw.Draw(img)\n",
    "        for x, y in keypoints:\n",
    "            draw.ellipse([x-r, y-r, x+r, y+r], c)\n",
    "        return img\n",
    "    \n",
    "    _, iw, ih, _ = images.shape\n",
    "    assert iw == ih\n",
    "    \n",
    "\n",
    "    tmp_images = images[indices]\n",
    "    tmp_features = features[indices]\n",
    "    predictions = model.predict(tmp_features)\n",
    "    kps = (predictions * (iw // 2)) + (iw // 2)\n",
    "\n",
    "    _, axes = plt.subplots(nrows=1, ncols=len(indices), figsize=[12,4])\n",
    "    if len(indices) == 1: axes = [axes]\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        img = PIL.Image.fromarray(images[idx])\n",
    "        axes[i].imshow(draw_keypoints(img, kps[i]))\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we:\n",
    "\n",
    "* load images and keypoints from folder structure\n",
    "* resize to 224x224 and save into numpy .npz file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subfolders within dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders_all = ['CAT_00', 'CAT_01', 'CAT_02', 'CAT_03', 'CAT_04', 'CAT_05', 'CAT_06']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get paths to all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_image_files_list(folders):\n",
    "    image_files_list = []\n",
    "    for folder in folders:\n",
    "        wild_path = os.path.join(dataset_location, folder, '*.jpg')\n",
    "        image_files_list.extend(sorted(glob.glob(wild_path)))\n",
    "    return image_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_all = build_image_files_list(folders_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Nb images:', len(image_paths_all))\n",
    "image_paths_all[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper to load keypoint data from *.cat* files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keypoints(path):    \n",
    "    \"\"\"\n",
    "        .cat is a single-line text file in format: 'nb_keypoints x1, y1, x2, y2, ...'\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        line = f.read().split()  # [nb_keypoints, x1, y1, x2, y2, ...]\n",
    "    keypoints_nb = int(line[0])  # int\n",
    "    keypoints_1d = np.array(line[1:], dtype=int)  # np.ndarray, [x1, y1, x2, y2, ...]\n",
    "    keypoints_xy = keypoints_1d.reshape((-1, 2))  # np.ndarray, [[x1, y1], [x2, y2], ...]\n",
    "    assert keypoints_nb == len(keypoints_xy)\n",
    "    assert keypoints_nb == 9                # always nine keypoints, eyes, nose, two ears\n",
    "    return keypoints_xy                     # np.ndarray, [[x1, y1], [x2, y2], ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper to draw keypoints on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(img, keypoints, r=2, c='red'):\n",
    "    \"\"\"Draw keypoints on PIL image\"\"\"\n",
    "    draw = PIL.ImageDraw.Draw(img)\n",
    "    for x, y in keypoints:\n",
    "        draw.ellipse([x-r, y-r, x+r, y+r], c)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open single image and load corresponding keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_path = image_paths_all[0]\n",
    "img = PIL.Image.open(example_path)\n",
    "kps = load_keypoints(example_path+'.cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show example keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(kps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(draw_keypoints(img.copy(), kps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper to scale image and keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_img_kps(image, keypoints, target_size):\n",
    "    width, height = image.size\n",
    "    ratio_w = width / target_size\n",
    "    ratio_h = height / target_size\n",
    "    \n",
    "    image_new = image.resize((target_size, target_size), resample=PIL.Image.LANCZOS)\n",
    "    \n",
    "    keypoints_new = np.zeros_like(keypoints)\n",
    "    keypoints_new[range(len(keypoints_new)), 0] = keypoints[:,0] / ratio_w\n",
    "    keypoints_new[range(len(keypoints_new)), 1] = keypoints[:,1] / ratio_h\n",
    "    \n",
    "    return image_new, keypoints_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2, kps2 = scale_img_kps(img, kps, target_size=224)\n",
    "display(draw_keypoints(img2.copy(), kps2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper to load and transform both input image and keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_keypoints(image_path, keypoints_path, target_size):\n",
    "    image = PIL.Image.open(image_path)\n",
    "    keypoints = load_keypoints(keypoints_path)\n",
    "    image_new, keypoints_new = scale_img_kps(image, keypoints, target_size)\n",
    "    return image, keypoints, image_new, keypoints_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show couple more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 21\n",
    "\n",
    "image, keypoints, image_new, keypoints_new = load_image_keypoints(\n",
    "    image_paths_all[idx], image_paths_all[idx]+'.cat', target_size=224)\n",
    "display(draw_keypoints(image.copy(), keypoints))\n",
    "display(draw_keypoints(image_new.copy(), keypoints_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_list = []\n",
    "keypoints_list = []\n",
    "for i, image_path in enumerate(image_paths_all):\n",
    "    _, _, image_new, keypoints_new = \\\n",
    "        load_image_keypoints(image_path, image_path+'.cat', target_size=224)\n",
    "    \n",
    "    image_arr = np.array(image_new)\n",
    "    # assert image_arr.shape == (224, 224, 3)\n",
    "    # assert 0 <= image_arr.min() <= 255\n",
    "\n",
    "    images_list.append(image_arr)\n",
    "    keypoints_list.append(keypoints_new)\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print('i:', i)\n",
    "        \n",
    "images = np.array(images_list)\n",
    "keypoints = np.array(keypoints_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('images.shape:', images.shape)\n",
    "print('images.dtype:', images.dtype)\n",
    "print('images.min()', images.min())\n",
    "print('images.max()', images.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some keypoints are outside of image (e.g. when cat ear is cropped out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('keypoints.shape:', keypoints.shape)\n",
    "print('keypoints.dtype:', keypoints.dtype)\n",
    "print('keypoints.min()', keypoints.min())\n",
    "print('keypoints.max()', keypoints.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "display(draw_keypoints(PIL.Image.fromarray(images[idx]).copy(), keypoints[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_npz = os.path.join(dataset_location, 'cats_224.npz')\n",
    "print(dataset_npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(dataset_npz, images=images, keypoints=keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess, Bottleneck Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we:\n",
    "\n",
    "* load 224x224 images and keypoints from .npz file\n",
    "* apply normalization to images\n",
    "* normalize keypoints to -1..1 range\n",
    "* pass through model to get bottleneck features (unused in this notebook)\n",
    "* save into second .npz file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_npz = os.path.join(dataset_location, 'cats_224.npz')\n",
    "print(dataset_npz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load(dataset_npz)\n",
    "images = npzfile['images']\n",
    "keypoints = npzfile['keypoints']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick one of the models, preprocessing and normalization may depend on model picked.\n",
    "\n",
    "Note that some setups use max pooling while other simply output last conv layer without pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'mobilenetv2_max'\n",
    "# preprocess_function = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "# model = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "#     input_shape=(224, 224, 3), alpha=1.0, include_top=False, pooling='max',\n",
    "#     weights='imagenet')  # output shape: (None, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'mobilenetv2'\n",
    "# preprocess_function = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "# model = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "#     input_shape=(224, 224, 3), alpha=1.0, include_top=False,\n",
    "#     weights='imagenet')  # output shape: (None, 7, 7, 1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'vgg16'\n",
    "# preprocess_function = tf.keras.applications.vgg16.preprocess_input\n",
    "# model = tf.keras.applications.vgg16.VGG16(\n",
    "#     input_shape=(224, 224, 3), include_top=False,\n",
    "#     weights='imagenet')  # output shape: (None, 7, 7, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'vgg19'\n",
    "# preprocess_function = tf.keras.applications.vgg19.preprocess_input\n",
    "# model = tf.keras.applications.vgg19.VGG19(\n",
    "#     input_shape=(224, 224, 3), include_top=False,\n",
    "#     weights='imagenet')  # output shape: (None, 7, 7, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'xception'\n",
    "# preprocess_function = tf.keras.applications.xception.preprocess_input\n",
    "# model = tf.keras.applications.xception.Xception(\n",
    "#     input_shape=(224, 224, 3), include_top=False,\n",
    "#     weights='imagenet')  # output shape: (None, 7, 7, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'densenet201'\n",
    "preprocess_function = tf.keras.applications.densenet.preprocess_input\n",
    "model = tf.keras.applications.densenet.DenseNet201(\n",
    "    input_shape=(224, 224, 3), include_top=False, # pooling='max',\n",
    "    weights='imagenet')  # (None, 7, 7, 1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'densenet201_max'\n",
    "# preprocess_function = tf.keras.applications.densenet.preprocess_input\n",
    "# model = tf.keras.applications.densenet.DenseNet201(\n",
    "#     input_shape=(224, 224, 3), include_top=False, pooling='max',\n",
    "#     weights='imagenet')  # (None, 1920)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert input into ImageNet format. This converts to float, scales and offsets to match distribution of ImageNet training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = preprocess_function(images)\n",
    "print('features.shape:', features.shape)\n",
    "print('features.dtype:', features.dtype)\n",
    "print('features.min()', features.min())\n",
    "print('features.max()', features.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck = model.predict(features, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('bottleneck.shape:', bottleneck.shape)\n",
    "print('bottleneck.dtype:', bottleneck.dtype)\n",
    "print('bottleneck.min()', bottleneck.min())\n",
    "print('bottleneck.max()', bottleneck.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert targets to range -1..1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = (keypoints - 112) / 112\n",
    "print('targets.shape:', targets.shape)\n",
    "print('targets.dtype:', targets.dtype)\n",
    "print('targets.min()', targets.min())\n",
    "print('targets.max()', targets.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_npz = os.path.join(dataset_location, 'processed_'+model_name+'.npz')\n",
    "print(processed_npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(processed_npz, features=features, targets=targets, bottleneck=bottleneck)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train End-to-End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mobilenetv2_max'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_npz = os.path.join(dataset_location, 'cats_224.npz')\n",
    "processed_npz = os.path.join(dataset_location, 'processed_'+model_name+'.npz')\n",
    "print(dataset_npz)\n",
    "print(processed_npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load(dataset_npz)\n",
    "images = npzfile['images']\n",
    "\n",
    "npzfile = np.load(processed_npz)\n",
    "features = npzfile['features']\n",
    "targets = npzfile['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 8000\n",
    "train_images = images[:split]\n",
    "train_features = features[:split]\n",
    "train_targets = targets[:split]\n",
    "valid_images = images[split:]\n",
    "valid_features = features[split:]\n",
    "valid_targets = targets[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "\n",
    "mobilenetv2 = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
    "    input_shape=(224, 224, 3), alpha=1.0, include_top=False,\n",
    "    weights='imagenet', input_tensor=X_inputs, pooling='max')\n",
    "\n",
    "X = tf.keras.layers.Dense(128, activation='relu')(mobilenetv2.layers[-1].output)\n",
    "X = tf.keras.layers.Dense(64, activation='relu')(X)\n",
    "X = tf.keras.layers.Dense(18, activation='linear')(X)\n",
    "X = tf.keras.layers.Reshape((9, 2))(X)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=X_inputs, outputs=X)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom callback for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallbackPlot(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        plot_images([10, 20, 30, 40, 50, 60], train_images, train_features)\n",
    "        plot_images([10, 20, 30, 40, 50, 60], valid_images, valid_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some cats before training. Most probably there won't be any keypoints shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images([10, 20, 30, 40, 50, 60], train_images, train_features)\n",
    "plot_images([10, 20, 30, 40, 50, 60], valid_images, valid_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "#   Callbacks\n",
    "#\n",
    "\n",
    "# tb_logdir = os.path.expanduser('~/logs/')\n",
    "# tb_counter  = len([log for log in os.listdir(tb_logdir) if 'cats' in log]) + 1\n",
    "# callback_tb = tf.keras.callbacks.TensorBoard(\n",
    "#     log_dir=tb_logdir + 'cats' + '_' + str(tb_counter), )\n",
    "\n",
    "callback_mc = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'model.h5', save_best_only=True, verbose=1)\n",
    "\n",
    "callback_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=5, verbose=1)\n",
    "\n",
    "# callback_plt = CallbackPlot()\n",
    "\n",
    "#\n",
    "#   Train\n",
    "#\n",
    "hist = model.fit(train_features, train_targets, epochs=50, batch_size=32, shuffle=True,\n",
    "  validation_data=(valid_features, valid_targets),\n",
    "  callbacks=[\n",
    "             #callback_tb,\n",
    "             callback_mc,\n",
    "             callback_lr,\n",
    "             #callback_plt,\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n",
    "ax1.plot(hist.history['loss'], label='loss')\n",
    "ax1.plot(hist.history['val_loss'], label='val_loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(hist.history['loss'], label='loss')\n",
    "ax2.plot(hist.history['val_loss'], label='val_loss')\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, .1)\n",
    "\n",
    "ax3.plot(hist.history['lr'], label='lr')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some cats from validation set - looks pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0, 50, 5):\n",
    "    plot_images([i for i in range(j, j+5)], valid_images, valid_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate more closely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 73\n",
    "plot_images([idx], valid_images, valid_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Regressor Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part trains only the regression part using bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'densenet201'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_npz = os.path.join(dataset_location, 'cats_224.npz')\n",
    "processed_npz = os.path.join(dataset_location, 'processed_'+model_name+'.npz')\n",
    "print(dataset_npz)\n",
    "print(processed_npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npzfile = np.load(dataset_npz)\n",
    "images = npzfile['images']\n",
    "\n",
    "npzfile = np.load(processed_npz)\n",
    "features = npzfile['features']\n",
    "targets = npzfile['targets']\n",
    "bottleneck = npzfile['bottleneck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 8000\n",
    "train_images = images[:split]\n",
    "train_features = features[:split]\n",
    "train_bottleneck = bottleneck[:split]\n",
    "train_targets = targets[:split]\n",
    "valid_images = images[split:]\n",
    "valid_features = features[split:]\n",
    "valid_bottleneck = bottleneck[split:]\n",
    "valid_targets = targets[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bottleneck.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inputs = tf.keras.layers.Input(shape=(7, 7, 1920))\n",
    "X = tf.keras.layers.Flatten()(X_inputs)\n",
    "X = tf.keras.layers.Dense(512, activation='relu')(X)\n",
    "X = tf.keras.layers.Dense(512, activation='relu')(X)\n",
    "X = tf.keras.layers.Dense(18, activation='linear')(X)\n",
    "X = tf.keras.layers.Reshape((9, 2))(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(inputs=X_inputs, outputs=X)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hist = model.fit(train_bottleneck, train_targets, epochs=50, batch_size=32, shuffle=True,\n",
    "  validation_data=(valid_bottleneck, valid_targets),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_keypoints(img, keypoints, r=2, c='red'):\n",
    "    \"\"\"Draw keypoints on PIL image\"\"\"\n",
    "    draw = PIL.ImageDraw.Draw(img)\n",
    "    for x, y in keypoints:\n",
    "        draw.ellipse([x-r, y-r, x+r, y+r], c)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_validate():\n",
    "\n",
    "    indices = [12, 21, 31, 40]\n",
    "    tmp_images = valid_images[indices]\n",
    "    tmp_inputs = valid_bottleneck[indices]\n",
    "    predictions = model.predict(tmp_inputs)\n",
    "\n",
    "    _, axes = plt.subplots(nrows=1, ncols=len(indices), figsize=[12,4])\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        img = PIL.Image.fromarray(valid_images[idx])\n",
    "        kps = predictions[i] * 112 + 112\n",
    "        axes[i].imshow(draw_keypoints(img, kps))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfgpu113]",
   "language": "python",
   "name": "conda-env-tfgpu113-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
